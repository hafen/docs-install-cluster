\section{Installing R Packages and Examples for Users}
\subsection{Background}

At this point, the DeltaRho stack, including Hadoop (a YARN based
distribution, such as Cloudera CDH5.x), R, and RHIPE should be installed.
However, at some sites the administrator may choose to only maintain
Hadoop, Java, Protocol Buffers, and R and rely on the end users to
install the various R packages, like RHIPE and rJava to their own
personal accounts.

Ideally, the versions of Linux R, and Java, are the same on both the
front-end R session server and the Hadoop cluster nodes.  Java in particular
plays a critical roll in RHIPE, and Java likes homogeneity.

\subsection{Install and Push}
If the administrator has not already installed RHIPE, you first
first download the package file by typing:

\begin{verbatim}
wget http://ml.stat.purdue.edu/rhipebin/Rhipe_0.75.1.4_cdh5.tar.gz
\end{verbatim}

Then launch R and type the following to install rJava and RHIPE:

\begin{verbatim}
install.packages("rJava")
install.packages("Rhipe_0.75.1.4_cdh5.tar.gz", repos=NULL, type="source")
\end{verbatim}

RHIPE is now installed. Each time you start an R session and you
want RHIPE to be available, type:

\begin{verbatim}
library(Rhipe)
rhinit()
\end{verbatim}

As a one-time configuration step, you push all the R packages you have
installed on the R session server, including RHIPE, onto the cluster HDFS.
First, you need the system administrator to configure a directory in HDFS
that is writable by you. We will assume the administrator has created
for you the writable directory \texttt{/user/loginname} using your
login name, and has done the same thing for other users.  Suppose in
\texttt{/user/loginname} you want to create a directory \texttt{bin}
on HDFS where you will push your installations on the R session
server. You can do this and carry out the push by

\begin{verbatim}
rhmkdir("/user/loginname/bin")
hdfs.setwd("/user/loginname/bin")
bashRhipeArchive("R.Pkg")
\end{verbatim}

\texttt{rhmkdir()} creates your directory \texttt{bin} in the directory \texttt{/user/loginname}.
\texttt{hdfs.setwd()} declares \texttt{/user/loginname/bin} to be the directory with your
choice of installations.  \texttt{bashRhipeArchive()} creates the actual archive of
your installations and names it as \texttt{R.Pkg}.

Each time your R code will require the installations on the HDFS, you
must in your R session run

\begin{verbatim}
library(Rhipe) rhinit()
rhoptions(zips = "/user/loginname/bin/R.Pkg.tar.gz")
rhoptions(runner = "sh ./R.Pkg/library/Rhipe/bin/RhipeMapReduce.sh")
\end{verbatim}


\subsection{Example: Housing Data}
\subsubsection{The Data}

The housing data consist of 7 monthly variables on housing sales from Oct
2008 to Mar 2014, which is 66 months. The measurements are for 2883 counties
in 48 U.S. states, excluding Hawaii and Alaska, and also for the District of
Columbia which we treat as a state with one county.
The data were derived from sales of housing units from Quandl's Zillow Housing
Data (www.quandl.com/c/housing).
A housing unit is a house, an apartment, a mobile home, a group of rooms, or a
single room that is occupied or intended to be occupied  as a
separate living quarter.

The variables are
\begin{itemize}
\item \verb|FIPS|: FIPS county code, an unique identifier for each U.S. county
\item \verb|county|: county name
\item \verb|state|: state abbreviation
\item \verb|date|: time of sale measured in months, from 1 to 66
\item \verb|units|: number of units sold
\item \verb|listing|: monthly median listing price (dollars per square foot)
\item \verb|selling|: monthly median selling price (dollars per square foot)
\end{itemize}

Many observations of the last three variables are missing: units 68\%, listing
7\%, and selling 68\%.

The number of measurements (including missing), is 7 x 66 x 2883 = 1,331,946.
So this is in fact a small dataset that could be analyzed in the standard
serial R. However, we can use them to illustrate how RHIPE R commands implement
Divide and Recombine. We simply pretend the data are large and complex, break
into subsets, and continuing on with D\&R. The small size let's you easily
pick up the data, follow along using the R commands in the tutorial, and
explore RHIPE yourself with other RHIPE R commands.

"housing.txt" is available in our DeltaRhodata Github repository of the
RHIPE documentation, or at:
\begin{verbatim}
https://raw.githubusercontent.com/delta-rho/docs-RHIPE/gh-pages/housing.txt
\end{verbatim}

The file is a table with 190,278 rows (66 months x 2883 counties) and
7 columns (the variables). The fields in each row are separated by a comma,
and there are no headers in the first line. Here are the first few lines of
the file:

\begin{verbatim}
01001,Autauga,AL,1,27,96.616541353383,99.1324
01001,Autauga,AL,2,28,96.856993190152,95.8209
01001,Autauga,AL,3,16,98.055555555556,96.3528
01001,Autauga,AL,4,23,97.747480735033,95.2189
01001,Autauga,AL,5,22,97.747480735033,92.7127
\end{verbatim}

\subsubsection{Write housing.txt to the HDFS}

To get started, we need to make \texttt{housing.txt} available as a text file within
the HDFS file system. This puts it in a place where it can be read into R, form
subsets, and write the subsets to the HDFS. This is similar to what we do
using R in the standard serial way; if we have a text file to read into R, we
move put it in a place where we can read it into R, for example, in the working
directory of the R session.

To set this up, the system administrator must do two tasks.
On the R session server, set up a home directory where you have write
permission; let's call it \texttt{/home/loginname}.
In the HDFS, the administrator does a similar thing, creates, say,
\texttt{/user/loginname} which is in the root directory.

Your first step, as for the standard R case, is to copy \texttt{housing.txt} to a
directory on the R-session server where your R session is running.
Suppose in your login directory you have created a directory \texttt{housing}
for your analysis of the housing data. You can copy \texttt{housing.txt} to

\begin{verbatim}
"/home/loginname/housing/"
\end{verbatim}

The next step is to get \texttt{housing.txt} onto the HDFS as a text file, so we can
read it into R on the cluster. There are Hadoop commands that could be used
directly to copy the file, but our promise to you is that you never need to
use Hadoop commands. There is a RHIPE function, \texttt{rhput()} that will do it
for you.

\begin{verbatim}
rhput("/home/loginname/housing/housing.txt", "/user/loginname/housing/housing.txt")
\end{verbatim}


The \texttt{rhput()} function takes two arguments.
The first is the path name of the R server file to be copied. The second
argument is the path name HDFS where the file will be written.
Note that for the HDFS, in the  directory \texttt{/user/loginname}
there is a directory \texttt{housing}. You might have created \texttt{housing}
already with the command

\begin{verbatim}
rhmkdir("/user/loginname/housing")
\end{verbatim}

If not, then \texttt{rhput()} creates the directory for you.

We can confirm that the housing data text file has been written to the HDFS
with the \texttt{rhexists()} function.

\begin{verbatim}
rhexists("/user/loginname/housing/housing.txt")
[1] TRUE
\end{verbatim}

We can use \texttt{rhls()} to get more information about files on the
HDFS. It is similar to the Unix command \texttt{ls}. For example,

\begin{verbatim}
rhls("/user/loginname/housing")
  permission         owner      group     size          modtime
1 -rw-rw-rw- loginname supergroup 7.683 mb 2014-09-17 11:11
                              file
/user/loginname/housing/housing.txt
\end{verbatim}

\subsubsection{Read and Divide by County}
Our division method for the housing data will be to divide by county,
so there will be 2883 subsets. Each subset will be a \texttt{data.frame} object with 4
column variables: \texttt{date}, \texttt{units}, \texttt{listing}, and \texttt{selling}.
\texttt{FIPS}, \texttt{state}, and \texttt{county} are not column variables because each has only one
value for each county; their values are added to the \texttt{data.frame} as
attributes.

The first step is to read each line of the file \texttt{housing.txt} into R. By
convention, RHIPE takes each line of a text file to be a key-value pair.
The line number is the key. The value is the data for the line, in our case
the 7 observations of the 7 variables of the data for one month and one county.

Each line is read as part of Map R code written by the user. The Map input
key-value pairs are the above line key-value pairs. Each line also has a Map
output key-value pair. The key identifies the county. \texttt{FIPS} could have been
enough to do this, but it is specified as a character vector with three
elements: the 3-vector values of \texttt{FIPS}, \texttt{state}, and \texttt{county}.
This is done so that later all three can be added to the subset \texttt{data.frame}.
The output value for each output key is the  observations of \texttt{date}, \texttt{units},
\texttt{listing}, and \texttt{selling} from the line for that key.

The Map output key-value pairs are the input key-value pairs for the Reduce R
code written by the user. Reduce assembles these into groups by key,
that is, the county. Then the Reduce R code is applied to the output
values of  each group collectively to create the subset \texttt{data.frame} object
for each county. Each row is the value of one Reduce input key-value pair:
observations of \texttt{date}, \texttt{units}, \texttt{listing}, and \texttt{selling} for one housing unit.
\texttt{FIPS}, \texttt{state}, and \texttt{county} are added to the \texttt{data.frame} as attributes.
Finally, Reduce writes
each subset \texttt{data.frame} object to a directory in the HDFS specified by the
user.  The subsets are written as Reduce output key-value pairs.
The output keys are the the values of \texttt{FIPS}. The output values are the county
\texttt{data.frame} objects.

\textbf{The RHIPE Manager: rhwatch()}

We begin with the RHIPE R function \texttt{rhwatch()}. It
runs the R code you write to specify
Map and Reduce operations, takes your specification of input and
output files, and manages key-value pairs for you.

The code for the county division is

\begin{verbatim}
mr1 <- rhwatch(
  map      = map1,
  reduce   = reduce1,
  input    = rhfmt("/user/loginname/housing/housing.txt", type = "text"),
  output   = rhfmt("/user/loginname/housing/byCounty", type = "sequence"),
  readback = FALSE
)
\end{verbatim}

Arguments \texttt{map} and \texttt{reduce} take your Map and Reduce R code, which will be
described below. \texttt{input} specifies the input to be the text file in the HDFS
that we put there earlier using \texttt{rhput()}. The file supplies input key-value
pairs for the Map code.  \texttt{output} specifies the file name
into which final output key-value pairs of the Reduce code that are written to
the HDFS. \texttt{rhwatch()} creates this file if it does not exist, or overwrites it
if it does not.

In our division by county here, the Reduce recombination outputs are the
2883 county \texttt{data.frame} R objects. They are a \texttt{list} object that describes the
key-value pairs: \texttt{FIPS} key and \texttt{data.frame} value. There is one \texttt{list} element
per pair; that element is itself a list with two elements, the \texttt{FIPS} key and
then the \texttt{data.frame} value.

The Reduce \texttt{list} output can also be written to the R global environment of
the R session. One use of this is analytic recombination in the R session
when the outputs are a small enough dataset. You can do this with the argument
\texttt{readback}.  If \texttt{TRUE}, the list is also written to the global environment.
If \texttt{FALSE}, it is not. If FALSE, it can be written latter using the RHIPE R
function \texttt{rhread()}.

\begin{verbatim}
countySubsets <- rhread("/user/loginname/housing/byCounty")
\end{verbatim}

Suppose you just want to look over the \texttt{byCounty} file on the HDFS just to see
if all is well, but that this can be done by looking at a small number of
key-value pairs, say 10. The code for this is

\begin{verbatim}
countySubsets <- rhread("/user/loginname/housing/byCounty", max = 10)
Read 10 objects(31.39 KB) in 0.04 seconds
\end{verbatim}

Then you can look at the list of length 10 in various was such as

\begin{verbatim}
keys <- unlist(lapply(countySubsets, "[[", 1))
keys
[1] "01013" "01031" "01059" "01077" "01095" "01103" "01121" "04001" "05019" "05037"
\end{verbatim}

\begin{verbatim}
attributes(countySubsets[[1]][[2]])
$names
[1] "date"             "units"            "listing"             "selling"
$row.names
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24
[25] 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48
[49] 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66
$state
[1] "AL"
$FIPS
[1] "01013"
$county
[1] "Butler"
$class
[1] "data.frame"
\end{verbatim}

\textbf{Map R Code}
The Map R code for the county division is

\begin{verbatim}
map1 <- expression({
  lapply(seq_along(map.keys), function(r) {
    line = strsplit(map.values[[r]], ",")[[1]]
    outputkey <- line[1:3]
    outputvalue <- data.frame(
      date = as.numeric(line[4]),
      units =  as.numeric(line[5]),
      listing = as.numeric(line[6]),
      selling = as.numeric(line[7]),
      stringsAsFactors = FALSE
    )
  rhcollect(outputkey, outputvalue)
  })
})
\end{verbatim}

Map has input key-value pairs, and output key-value pairs. Each pair has an
identifier, the key, and numeric-categorical information, the value.
The Map R code is applied to each input key-value pair, producing one
output key-value pair. Each application of the Map code to a
key-value pair is carried out by a mapper, and there are many mappers running
in parallel without communication (embarrassingly parallel) until the Map job
completes.

RHIPE creates input key-value pair \texttt{list} objects, \texttt{map.keys} and
\texttt{map.values}, based on information that it has.
Let \texttt{r} be an integer from 1 to the number of input key-value pairs.
\texttt{map.values[[r]]} is the value for key \texttt{map.keys[[r]]}.
The housing data inputs come from a text file in the HDFS, housing.txt,
By RHIPE convention, for a text file, each Map input key is a text file line
number, and the corresponding  Map input value is the observations in the line,
read into R as a single text string.
In our case each line value is the observations of the 7 county variables for the line.

This Map code is really a "for loop" with \texttt{r} as the looping variable,
but is done by \texttt{lapply()} because it is
in general faster than \texttt{for r in 1:length(map.keys)}.
The loop proceeds through the input keys, specified by the first argument of
\texttt{lapply}.  The second argument of the above \texttt{lapply} defines the Map expression
with the argument \texttt{r}, an index for the Map keys and values.

The function \texttt{strsplit()} splits each character-string line input value
into the individual observations of the text line. The result, \texttt{line},
is a \texttt{list} of length one whose element is a \texttt{character vector} whose elements
are the line observations. In our case, the
observations are a \texttt{character vector} of length 7, in order:
\texttt{FIPS}, \texttt{county}, \texttt{state}, \texttt{date}, \texttt{units}, \texttt{listing},
\texttt{selling}.

Next we turn to the Map output key-value pairs.
\texttt{outputkey} for each text line is a character vector of length 3 with \texttt{FIPS},
\texttt{county}, and \texttt{state}. \texttt{utputvalue} is a \texttt{data.frame} with one row
and 4 columns, the observations of \texttt{date}, \texttt{units}, \texttt{listing}, and
\texttt{selling}, each a \texttt{numeric} object.

The argument of \texttt{data.frame}, \texttt{stringsAsFactors}, is
is given the value \texttt{FALSE}. This leaves character vectors in the \texttt{data.frame}
as is, and does on convert to a \texttt{factor}.

The RHIPE function \texttt{rhcollect()} forms a Map output key-value pair for each
line, and writes the results to the HDFS as a key-value pair \texttt{list} object.

\textbf{Reduce R Code}
The Reduce R code for the county division is

\begin{verbatim}
reduce1 <- expression(
  pre = {
    reduceoutputvalue <- data.frame()
  },
  reduce = {
    reduceoutputvalue <- rbind(reduceoutputvalue, do.call(rbind, reduce.values))
  },
  post = {
    reduceoutputkey <- reduce.key[1]
    attr(reduceoutputvalue, "location") <- reduce.key[1:3]
    names(attr(reduceoutputvalue, "location")) <- c("FIPS","county","state")
    rhcollect(reduceoutputkey, reduceoutputvalue)
  }
)
\end{verbatim}

The output key-value pairs of Map are the input key-value pairs to Reduce.
The first task of Reduce is to group its input key-value pairs by unique key.
The Reduce R code is applied to the key-value pairs of each group by a
reducer. The number of groups varies in applications from just one, with a
single Reduce output, to many.
For multiple groups, the reducers run in parallel, without communication,
until the Reduce job completes.

RHIPE creates two list objects \texttt{reduce.key} and \texttt{reduce.values}.
Each element of \texttt{reduce.key} is the key for one group, and the corresponding
element of \texttt{reduce.values} has the values for the group to which the Reduce
code is applied.  Now in our case, the key is county and the values are the
observations of  \texttt{date}, \texttt{units}, \texttt{listing}, and \texttt{selling}
for the all housing units in the county.

Note the Reduce code has a certain structure: expressions \texttt{pre}, \texttt{reduce},
and \texttt{post}. In our case \texttt{pre} initializes \texttt{reduceoutputvalue} to a
\texttt{data.frame()}. \texttt{reduce} assembles the county \texttt{data.frame} as the
reducer receives the values through \texttt{rbind(reduceoutputvalue, do.call(rbind,
reduce.values))}; this uses \texttt{rbind()} to add rows to the \texttt{data.frame} object.
\texttt{post} operates further on the result of \texttt{reduce}. In our case it first assigns
the observation of \texttt{FIPS} as the key. Then it adds \texttt{FIPS},\texttt{county}, and
\texttt{state} as \texttt{attributes}. Finally the RHIPE function
\texttt{rhcollect()} forms a Reduce output key-value pair \texttt{list}, and writes it to the
HDFS.

\subsubsection{Compute County Min, Median, Max}

With the county division subsets now in the HDFS we will illustrate using them
to carry out D\&R with a very simple recombination procedure based on a
summary statistic for each county of the variable \texttt{listing}.
We do this for simplicity of explanation of how RHIPE works.
However, we emphasize that in practice, initial analysis would
almost always involve comprehensive analysis of both the detailed data for all
subset variables and summary statistics based on the detailed data.

Our summary statistic consists of the minimum, median, and maximum of
\texttt{listing}, one summary for each county. Map R code computes the statistic.
The output key of Map, and therefore the input key for Reduce is \texttt{state}.
The Reduce R code creates a \texttt{data.frame} for each state
where the columns are \texttt{FIPS}, \texttt{county}, \texttt{min}, \texttt{median}, and \texttt{max}.
So our example illustrates a scenario where we create summary statistics, and
then analyze the results. This is an analytic recombination. In addition, we
suppose that in this scenario the summary statistic dataset is small enough to
analyze in the standard serial R. This is not uncommon in practice even when
the raw data are very large and complex.

\textbf{The RHIPE Manager: rhwatch()}

Here is the code for \texttt{rhwatch()}.

\begin{verbatim}
CountyStats <- rhwatch(
  map      = map2,
  reduce   = reduce2,
  input    = rhfmt("/user/loginname/housing/byCounty", type = "sequence"),
  output   = rhfmt("/user/loginname/housing/CountyStats", type = "sequence"),
  readback = TRUE
)
\end{verbatim}

Our Map and Reduce code, \texttt{map2} and \texttt{reduce2}, is given to the arguments
\texttt{map} and \texttt{reduce}. The code will be will be discussed later.

The input key-value pairs for Map, given to the argument \texttt{input},
are our county subsets which were written to the HDFS directory
\texttt{/user/loginname/housing}  as the key-value pairs \texttt{list} object \texttt{byCounty}.
The final output key-value pairs for Reduce, specified by the argument
\texttt{output}, will be written to the \texttt{list} object \texttt{CountyStats} in the same
directory as the subsets. The keys are the states, and the values are the
\texttt{data.frame} objects for the states.

The argument \texttt{readback} is given the value TRUE, which means \texttt{CountyStats} is
also written to the R global environment of the R session. We do this because
our scenario is that analytic recombination is done in R.

The argument \texttt{mapred.reduce.tasks} is given the value 10, as in our use of it
to create the county subsets.


\textbf{The Map R Code}

The Map R code is
\begin{verbatim}
map2 <- expression({
  lapply(seq_along(map.keys), function(r) {
    outputvalue <- data.frame(
      FIPS = map.keys[[r]],
      county = attr(map.values[[r]], "location")["county"],
      min = min(map.values[[r]]$listing, na.rm = TRUE),
      median = median(map.values[[r]]$listing, na.rm = TRUE),
      max = max(map.values[[r]]$listing, na.rm = TRUE),
      stringsAsFactors = FALSE
    )
    outputkey <- attr(map.values[[r]], "location")["state"]
    rhcollect(outputkey, outputvalue)
  })
})
\end{verbatim}

\texttt{map.keys} is the Map input keys, the county subset identifiers \texttt{FIPS}.
\texttt{map.values} is the Map input values, the county subset \texttt{data.frame}
objects. The \texttt{lapply()} loop goes through all subsets, and the looping
variable is \texttt{r}. Each stage of the loop creates one output key-value pair,
\texttt{outputkey} and \texttt{outputvalue}.
\texttt{outputkey} is the observation of \texttt{state}. \texttt{outputvalue} is a \texttt{data.frame}
with one row that has the variables \texttt{FIPS}, \texttt{county}, \texttt{min}, \texttt{median}, and
\texttt{max} for county \texttt{FIPS}. \texttt{rhcollect(outputkey, outputvalue)} emits the pairs
to reducers, becoming the Reduce input key-value pairs.

\textbf{The Reduce R Code}
The Reduce R code for the \texttt{listing} summary statistic is

\begin{verbatim}
reduce2 <- expression(
  pre = {
    reduceoutputvalue <- data.frame()
  },
  reduce = {
    reduceoutputvalue <- rbind(reduceoutputvalue, do.call(rbind, reduce.values))
  },
  post = {
    rhcollect(reduce.key, reduceoutputvalue)
  }
)
\end{verbatim}

The first task of Reduce is to group its input key-value pairs by unique key,
in this case by \texttt{state}. The Reduce R code is applied to the key-value pairs
of each group by a reducer.

Expression \texttt{pre}, initializes \texttt{reduceoutputvalue} to a
\texttt{data.frame()}. \texttt{reduce} assembles the state \texttt{data.frame} as the
reducer receives the values through \texttt{rbind(reduceoutputvalue, do.call(rbind,
reduce.values))}; this uses \texttt{rbind()} to add rows to the \texttt{data.frame} object.
\texttt{post} operates further on the result of \texttt{reduce}; \texttt{rhcollect()} forms a Reduce
output key-value pair for each state. RHIPE then writes the Reduce output
key-value pairs to the HDFS.

Recall that we told RHIPE in \texttt{rhwatch()} to also write the Reduce output
to \texttt{CountyStats} in both the R server global environment. There, we can have a
look at the results to make sure all is well. We can look at a summary

\begin{verbatim}
str(CountyStats)
List of 49
 $ :List of 2
  ..$ : Named chr "AL"
  .. ..- attr(*, "names")= chr "state"
  ..$ :'data.frame':    64 obs. of  5 variables:
  .. ..$ FIPS  : chr [1:64] "01055" "01053" "01051" "01049" ...
  .. ..$ county: chr [1:64] "Etowah" "Escambia" "Elmore" "DeKalb" ...
  .. ..$ min   : num [1:64] 62.1 60.4 94.7 59.2 41.2 ...
  .. ..$ median: num [1:64] 67.6 66.2 99.2 71.9 50.6 ...
  .. ..$ max   : num [1:64] 77.8 79.8 102.2 82.3 60.4 ...
 $ :List of 2
  ..$ : Named chr "AR"
  .. ..- attr(*, "names")= chr "state"
  ..$ :'data.frame':    71 obs. of  5 variables:
  .. ..$ FIPS  : chr [1:71] "05025" "05023" "05021" "05019" ...
  .. ..$ county: chr [1:71] "Cleveland" "Cleburne" "Clay" "Clark" ...
  .. ..$ min   : num [1:71] 46.2 99.9 28.1 61.6 58.5 ...
  .. ..$ median: num [1:71] 60.2 108.2 38.7 67.3 82.1 ...
  .. ..$ max   : num [1:71] 73.5 125 48.8 72.7 117.4 ...
......
\end{verbatim}

We can look at the first key-value pair

\begin{verbatim}
CountyStats[[1]][[2]]
[[1]]
state
 "AL"
\end{verbatim}

We can look at the \texttt{data.frame} for state "AL"

\begin{verbatim}
head(CountyStats[[1]][[2]])
        min    median       max  FIPS     county
1  34.88372  51.88628  73.46257 01093     Marion
2  94.66667  99.20582 102.23077 01051     Elmore
3  83.93817  88.59977  94.67041 01031     Coffee
4  92.87617  97.53306 105.71429 01125 Tuscaloosa
5  60.34774  72.46377  93.53741 01027       Clay
6 108.97167 119.66207 128.13390 01117     Shelby
\end{verbatim}
