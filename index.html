<html lang="en">
<head>
  <meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta name="generator" content="pandoc" />



  <title>DeltaRho Installation on a Cluster</title>

    <script src="assets/jquery-1.11.0/jquery.min.js"></script>
  <link href="assets/bootstrap-3.3.2/css/bootstrap.min.css" rel="stylesheet" />
  <script src="assets/bootstrap-3.3.2/js/bootstrap.min.js"></script>
  <script src="assets/bootstrap-3.3.2/shim/html5shiv.min.js"></script>
  <script src="assets/bootstrap-3.3.2/shim/respond.min.js"></script>
  <link href="assets/highlight-8.4/tomorrow.css" rel="stylesheet" />
  <script src="assets/highlight-8.4/highlight.pack.js"></script>
  <link href="assets/fontawesome-4.3.0/css/font-awesome.min.css" rel="stylesheet" />
  <script src="assets/stickykit-1.1.1/sticky-kit.min.js"></script>
  <script src="assets/jqueryeasing-1.3/jquery.easing.min.js"></script>
  <link href="assets/packagedocs-0.0.1/pd.css" rel="stylesheet" />
  <script src="assets/packagedocs-0.0.1/pd.js"></script>
  <script src="assets/packagedocs-0.0.1/pd-sticky-toc.js"></script>





  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
</head>

<body>


  <header class="navbar navbar-white navbar-fixed-top" role="banner" id="header">
    <div class="container">
      <div class="navbar-header">
        <button class="navbar-toggle" type="button" data-toggle="collapse" data-target=".navbar-collapse">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
                <span class="navbar-brand">
<a href="http://deltarho.org"> <img src='figures/icon.png' alt='deltarho icon' width='30px' height='30px' style='margin-top: -3px;'> </a>
        </span>
                <a href="#content-top" class="navbar-brand page-scroll">
        DeltaRho Installation on a Cluster
        </a>
      </div>
            <nav class="collapse navbar-collapse" role="navigation">
        <ul class="nav nav-pills pull-right">
<li class="active">
<a href='install-redhat.html'>Redhat</a>
</li>
<li>
<a href='install-ubuntu.html'>Ubuntu</a>
</li>
        </ul>
      </nav>
          </div>
  </header>

  <!-- Begin Body -->
  <div class="container">
    <div class="row">
            <div class="col-md-3" id="sidebar-col">
        <div id="toc">
          <ul>
          <li><a href="#tessera-installation-on-red-hat-enterprise-linux">Tessera Installation on Red Hat Enterprise Linux</a></li>
          <li><a href="#the-r-rhipe-hadoop-setting">The R, RHIPE, Hadoop Setting</a><ul>
          <li><a href="#overview">Overview</a></li>
          <li><a href="#the-r-session-server-and-rstudio">The R-Session Server and RStudio</a></li>
          <li><a href="#the-remote-computer">The Remote Computer</a></li>
          <li><a href="#where-are-the-data-analyzed">Where Are the Data Analyzed</a></li>
          <li><a href="#a-few-basic-hadoop-features">A Few Basic Hadoop Features</a></li>
          </ul></li>
          <li><a href="#deltarho-stack-cluster-installation-guide">DeltaRho Stack Cluster Installation Guide</a><ul>
          <li><a href="#introduction">Introduction</a></li>
          <li><a href="#example-configuration">Example Configuration</a></li>
          <li><a href="#packages">Packages</a></li>
          <li><a href="#installation">Installation</a></li>
          <li><a href="#configuration">Configuration</a></li>
          <li><a href="#hadoop-pre-run-setup">Hadoop Pre-run Setup</a></li>
          <li><a href="#starting-the-hadoop-cluster">Starting the Hadoop Cluster</a></li>
          <li><a href="#checking-the-status-of-the-hadoop-cluster">Checking the status of the Hadoop Cluster</a></li>
          <li><a href="#notes">Notes</a></li>
          </ul></li>
          <li><a href="#installing-r-packages-and-examples-for-users">Installing R Packages and Examples for Users</a><ul>
          <li><a href="#background">Background</a></li>
          <li><a href="#install-and-push">Install and Push</a></li>
          <li><a href="#example-housing-data">Example: Housing Data</a></li>
          </ul></li>
          </ul>
        </div>
      </div>
      <div class="col-md-9" id="content-col">

<div id="content-top"></div>
<div id="tessera-installation-on-red-hat-enterprise-linux" class="section level1">
<h1>Tessera Installation on Red Hat Enterprise Linux</h1>
<p>This is a guide to installing the Tessera stack consisting of Hadoop, RHIPE, datadr, trelliscope and other supporting packages on a multi-node cluster running 64-bit Red Hat Enterprise Linux version 6.x. These instructions should also work with few modifications on CentOS 6. An Ubuntu version of this installation guide is available at <a href="http://tessera.io/docs-install-cluster/install-ubuntu.html" class="uri">http://tessera.io/docs-install-cluster/install-ubuntu.html</a>.</p>
</div>
<div id="the-r-rhipe-hadoop-setting" class="section level1">
<h1>The R, RHIPE, Hadoop Setting</h1>
<div id="overview" class="section level2">
<h2>Overview</h2>
<p>The setting has three components: remote computer, one or more Unix R-session servers, and a Unix Hadoop cluster. The second two components are running R and RHIPE. You work on the remote computer, say your laptop, and login to an R-session server. This is home base, where you do all of your programming of R and RHIPE R commands. The R commands you write for division, application of analytic methods, and recombination that are destined for Hadoop on the cluster are passed along by RHIPE R commands.</p>
<p>The remote computer is typically for you to maintain. The R-session servers require IT staff to help install software, configure, and maintain. However you install packages too on the R-session servers, just you do when you want to use an R CRAN package in R. There is an extra task though; you want packages you install to be pushed up the Hadoop cluster so they can be used there too. Except for this push by you, the Hadoop cluster is the domain of the systems administrators who must, among other tasks, install Hadoop.</p>
</div>
<div id="the-r-session-server-and-rstudio" class="section level2">
<h2>The R-Session Server and RStudio</h2>
<p>Now the R-session server can be separate from the Hadoop cluster, handling only R sessions, or it can be one of the servers on the Hadoop cluster. If it is on the Hadoop cluster, there must be some precautions taken in the Hadoop configuration to protect the programming of the R session. This is needed because the RHIPE Hadoop jobs compete with the R sessions. There are never full guarantees though, so “safe mode” is separate R session servers. The last thing you want is for R sessions to get bogged down. If the cluster option is chosen, then you want to mount a file server on the cluster that contains the files associated with the R session such as .RData and files read into to R or written by R.</p>
<p>There is a vast segment of the R community that uses RStudio, for good reason. RStudio can join the setting. You have RStudio server installed on the R-session servers by system administrators. A web browser on the R server runs the RStudio interface which is accessed by you on your remote device via the remote login.</p>
</div>
<div id="the-remote-computer" class="section level2">
<h2>The Remote Computer</h2>
<p>The remote computer is just a communication device, and does not carry out data analysis, so it can run any operating system, such as Windows. This is especially important for teaching, since Windows labs are typically very plentiful at academic institutions, but Unix labs much less so. Whatever the operating system, a common communication protocol that is used is the SSH protocol. SSH is typically used to log into a remote machine and execute commands or to transfer files. But a critical capability of it for our purposes here is that it supports both your R session command-line window, showing both input and output, and a separate window to show graphics.</p>
</div>
<div id="where-are-the-data-analyzed" class="section level2">
<h2>Where Are the Data Analyzed</h2>
<p>Obviously, much data analysis is carried out by Hadoop on the Hadoop cluster. Your R commands are given to RHIPE, passed along to Hadoop, and the outputs are written by Hadoop to the HDFS.</p>
<p>But in many analyses of larger and more complex data, it is common to have (1) outputs of a recombination method that constitute a relatively small dataset, and (2) the outputs are further analyzed as part of the overall analysis. If they are small enough to be readily analyzed in your R session, then for sure that is where you want to be. RHIPE commands allow you to write the recombination outputs from the HDFS to the R global environment of your R session. They become a dataset in .RData. While programming R and RHIPE is easy, it is not as easy as plain old serial R. The point is that a lot of data analysis can be carried out in just R even when the data are large and complex.</p>
</div>
<div id="a-few-basic-hadoop-features" class="section level2">
<h2>A Few Basic Hadoop Features</h2>
<p>The two principal computational operations of Hadoop are Map and Reduce. The first runs parallel computations on subsets without communication among them. The second can compute across subset outputs. So Map carries out the analytic method computation. Reduce takes the outputs from Map and runs the recombination computation. A division is typically carried out both by Map and Reduce, sometimes each used several times, and can occur as part of the reading of the data into R at the start of the analysis.</p>
<p>Usage of Map and Reduce involves the critical Hadoop element of key-value pairs. We give one instance here. The Map operation, instructed by the analyst R code, puts a key on each subset output. This forms a key-value pair with the output as the value. Each output can have a unique key, or each key can be given to many outputs, or all outputs can have the same key. When Reduce is given the Map outputs, it assembles the key-value pairs by key, which forms groups, and then the R recombination code is applied to the values of each group independently; so the running of the code on the different groups is embarrassingly parallel. This framework provides substantial flexibility for the recombination method.</p>
<p>Hadoop attempts to optimize computation in a number of ways. One example is Map. Typically, there are vastly more subsets than cores on the cluster. When Map finishes the application of the analytic method to a subset on a core, Hadoop seeks to assign a subset on the same node as the core to avoid transmission of the subset across the network connecting the nodes, which is more time consuming.</p>
</div>
</div>
<div id="deltarho-stack-cluster-installation-guide" class="section level1">
<h1>DeltaRho Stack Cluster Installation Guide</h1>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<<<<<<< HEAD
<p>This is a guide to installing the DeltaRho stack consisting of Hadoop, RHIPE, datadr, trelliscope and other supporting packages on a multi-node cluster. This guide is intended to be used by the Systems Administrator to quickly install and configure the software necessary to run the full DeltaRho stack. It is not focused on tuning and optimally configuring the DeltaRho stack. It simplifies and generalizes certain aspects of the installation. The Systems Administrator is encouraged to consult individual package documentation for further tuning later. In this guide we assume 64-bit Red Hat Enterprise Linux version 6.x is the operating system installed on he cluster, though these instructions should work with few modifications on CentOS and Fedora. An Ubuntu version of this installation guide is available at <a href="http://deltarho.org/docs-install-cluster/install-ubuntu.html" class="uri">http://deltarho.org/docs-install-cluster/install-ubuntu.html</a>.</p>
=======
<p>This guide is intended to be used by the Systems Administrator to quickly install and configure the software necessary to run the full Tessera stack on a multi-node cluster. It is not focused on tuning and optimally configuring the Tessera stack. Most components of Tessera require no tuning. However, the Systems Administrator is encouraged to consult the Cloudera Hadoop documentation for further tuning of Hadoop and HDFS.</p>
>>>>>>> upstream/gh-pages
</div>
<div id="example-configuration" class="section level2">
<h2>Example Configuration</h2>
<p>Running the DeltaRho stack on a Hadoop cluster requires each Hadoop node to be configured to fill one or more of the roles of ResourceManager, NameNode, Secondary NameNode, DataNode, and NodeManager. The role descriptions are as follows.</p>
<ul>
<li><p>NameNode - Manages the directory tree and metadata of the Hadoop File System (HDFS), a distributed virtual file system that allows data to be replicated and stored across many nodes in the cluster.</p></li>
<li><p>SecondaryNameNode - Offloads HDFS checkpoint support for the NameNode. It is not a NameNode failover or backup as the name may imply.</p></li>
<li><p>ResourceManager - Schedules and issues map reduce jobs for NodeManagers across the cluster.</p></li>
<li><p>DataNode - Stores data on the local drives of nodes as part of the distributed HDFS. DataNodes are almost always also NodeManagers.</p></li>
<li><p>NodeManager - Executes the map and reduce jobs issued by the ResourceManager. NodeManagers are almost always also DataNodes.</p></li>
</ul>
<p>There can be just one NameNode, one ResourceManager, and one Secondary NameNode, but there will be many DataNodes and NodeManagers. The node acting as NameNode cannot also be the Secondary NameNode, but could perform one or more of other roles of ResourceManager, DataNode, and NodeManager with a potential performance penalty. Most cluster nodes will be compute nodes with multiple hard drives and thus will run both the DataNode and NodeManager services.</p>
<p>For our example we will have a single front-end server that acts as the R-session server. It is where we will run R, which in turn runs RHIPE. The front-end server is aware of our Hadoop configuration and can issue Hadoop commands, though it isn’t technically a Hadoop node itself. The front-end server will also be our Shiny server and optional Rstudio server. We will have five nodes in our example Hadoop cluster. Each of the Hadoop nodes in the cluster has a fully qualified domain name of the form <code>nodeNNN.example.com</code>. In our example,</p>
<ul>
<li><p><code>node001.example.com</code> will be the NameNode and the ResourceManager.</p></li>
<li><p><code>node002.example.com</code> will be the Secondary NameNode, a NodeManager, and a DataNode.</p></li>
<li><p><code>node003.example.com</code> to <code>node005.example.com</code> will be NodeManagers and DataNodes.</p></li>
<li><p><code>frontend.example.com</code> will be the front end R-session server, the Shiny server, and optional Rstudio Server.</p></li>
</ul>
</div>
<div id="packages" class="section level2">
<h2>Packages</h2>
<div id="required-packages" class="section level3">
<h3>Required Packages</h3>
<p>The DeltaRho stack consists of some packages provided by the DeltaRho group and some provided by other sources.<br /> <strong>Packages provided by the DeltaRho group</strong></p>
<ol style="list-style-type: decimal">
<li><p>RHIPE - 0.75.1.4</p></li>
<li><p>datadr</p></li>
<li><p>trelliscope</p></li>
</ol>
<p><strong>Packages provided by others</strong></p>
<ol style="list-style-type: decimal">
<li><p>Sun/Oracle Java - 7</p></li>
<li><p>Protocol Buffers - 2.5.0</p></li>
<li><p>Cloudera Hadoop version cdh5.3.3</p></li>
<li><p>R - 3.1.3</p></li>
<li><p>rJava - 0.9-6</p></li>
<li><p>R packages - codetools, ggplot2, lattice, boot, shiny, devtools</p></li>
<li><p>Shiny server</p></li>
</ol>
</div>
<div id="optional-packages" class="section level3">
<h3>Optional Packages</h3>
<p>These optional packages can be installed along with the DeltaRho stack providing convenience for the analyst.</p>
<ol style="list-style-type: decimal">
<li>Rstudio server - Rstudio server creates a persistent interactive R session for the analyst by running R on a remote server and providing access through a the analyst’s local browser.</li>
</ol>
</div>
</div>
<div id="installation" class="section level2">
<h2>Installation</h2>
<p>Some packages need only be installed on the Hadoop nodes, some need only be installed on the frontend server, and some must be installed on both. This is specified on a per-package basis below. This guide is command line friendly.</p>
<div id="compilers-and-development-tools-all-servers" class="section level3">
<h3>Compilers and development tools (all servers)</h3>
<p>Some of the software to be installed must be compiled from source, so a typical set of compilers (gcc, gfortran) and development tools must be installed first. This can be done by typing:</p>
<pre><code>sudo yum groupinstall &quot;Development tools&quot;</code></pre>
</div>
<div id="java-all-servers" class="section level3">
<h3>Java (all servers)</h3>
<p>(<a href="http://www.java.com/en/" class="uri">http://www.java.com/en/</a>)<br />Oracle Java is downloaded from www.java.com for 64-bit RHEL by browsing to:</p>
<p><a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html" class="uri">http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html</a></p>
<p>Check the “Accept License Agreement” radio box, then click the jdk-7u79-linux-x64.rpm link (or newer version if available) and save. Cd to the directory where it was downloaded and type the following to install the Java JDK, changing the filename to match the filename downloaded if necessary:</p>
<pre><code>sudo rpm -ivh jdk-7u79-linux-x64.rpm</code></pre>
<p>Next, tell Red Hat that the newly installed Oracle Java is the default version to use, even if other versions of Java like OpenJDK were previously installed:</p>
<pre><code>sudo /usr/sbin/alternatives --install /usr/bin/java java /usr/java/latest/bin/java 200005
sudo /usr/sbin/alternatives --install /usr/bin/javac javac /usr/java/latest/bin/javac 200005
sudo /usr/sbin/alternatives --install /usr/bin/javadoc javadoc /usr/java/latest/bin/javadoc 200005
sudo /usr/sbin/alternatives --install /usr/bin/javah javah /usr/java/latest/bin/javah 200005
sudo /usr/sbin/alternatives --install /usr/bin/javap javap /usr/java/latest/bin/javap 200005
sudo /usr/sbin/alternatives --install /usr/bin/javaws javaws /usr/java/latest/bin/javaws 200005
sudo /usr/sbin/alternatives --install /usr/lib64/mozilla/plugins/libjavaplugin.so libjavaplugin.so.x86_64 /usr/java/latest/jre/lib/amd64/libnpjp2.so 200005
sudo /usr/sbin/alternatives --install /usr/bin/jar jar /usr/java/latest/bin/jar 200005</code></pre>
</div>
<div id="protocol-buffers-all-servers" class="section level3">
<h3>Protocol Buffers (all servers)</h3>
<p>(<a href="https://code.google.com/p/protobuf/" class="uri">https://code.google.com/p/protobuf/</a>)<br />Install Protocol Buffers. It is important that version 2.5.0 be installed and not a newer or older version. Note that certain other packages to be installed later rely on the pkg-config program, which should already exist on Red Hat. The pkg-config program expects that newly installed programs register themselves by placing <em>programname</em>.pc files in /usr/lib64/pkgconfig, which is why there is a <code>--libdir=/usr/lib64</code> in the instructions below. To install, first cd to a directory where the software can be downloaded and built, and type the following.</p>
<pre><code>wget https://github.com/google/protobuf/archive/v2.5.0.zip
unzip v2.5.0.zip
cd protobuf-2.5.0
./autogen.sh
./configure --prefix=/usr/local --libdir=/usr/lib64
make
sudo make install</code></pre>
</div>
<div id="hadoop-all-servers" class="section level3">
<h3>Hadoop (all servers)</h3>
<p>(<a href="http://www.cloudera.com/content/cloudera/en/products-and-services/cdh.html" class="uri">http://www.cloudera.com/content/cloudera/en/products-and-services/cdh.html</a>)<br />We use the Hadoop distribution built by Cloudera. It is a Hadoop MRv2 / YARN implementation. Download the Cloudera repo definition and install it so we can install Cloudera packages directly using yum. Then update yum so it is aware of the packages available on the Cloudera repository.</p>
<pre><code>wget http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/cloudera-cdh5.repo
sudo cp cloudera-cdh5.repo /etc/yum.repos.d/
sudo yum update</code></pre>
<p>We install different Hadoop packages on each node based on the Hadoop roles played by that node. After installing each set of packages we will configure them to be started automatically at boot time using <code>chkconfig</code>.</p>
<p>node001.example.com:</p>
<pre><code>sudo yum install hadoop-yarn-resourcemanager hadoop-hdfs-namenode hadoop-client
sudo chkconfig hadoop-yarn-resourcemanager on
sudo chkconfig hadoop-hdfs-namenode on</code></pre>
<p>node002.example.com:</p>
<pre><code>sudo yum install hadoop-hdfs-secondarynamenode hadoop-yarn-nodemanager
sudo yum install hadoop-hdfs-datanode hadoop-mapreduce
sudo chkconfig hadoop-hdfs-secondarynamenode on
sudo chkconfig hadoop-yarn-nodemanager on
sudo chkconfig hadoop-hdfs-datanode on</code></pre>
<p>node003.example.com through node005.example.com:</p>
<pre><code>sudo yum install hadoop-yarn-nodemanager hadoop-hdfs-datanode hadoop-mapreduce
sudo chkconfig hadoop-yarn-nodemanager on
sudo chkconfig hadoop-hdfs-datanode on</code></pre>
<p>frontend.example.com (doesn’t run any hadoop services, but needs the hadoop commands):</p>
<pre><code>sudo yum install hadoop-client</code></pre>
<p>As part of the above steps, three new users and groups named yarn, hdfs, and mapred will be added to the system. All three users are added to the new group hadoop.</p>
<p>Configuration of Hadoop settings is done in another section, but for now we will prepare the configuration directory and set it as the default Hadoop configuration on the system. On each of node001 through node005 and on the frontend, execute the following. We choose the configuration directory name of “conf.deltarho” in this guide:</p>
<pre><code>sudo cp -r /etc/hadoop/conf.empty /etc/hadoop/conf.deltarho
sudo /usr/sbin/alternatives --install /etc/hadoop/conf hadoop-conf /etc/hadoop/conf.deltarho 50
sudo /usr/sbin/alternatives --set hadoop-conf /etc/hadoop/conf.deltarho</code></pre>
<p>Add Hadoop environment variables and an updated path to the user environment for future logins. This assumes users use an sh/ksh/bash/zsh based login shell.</p>
<pre><code>echo &quot;export HADOOP=/usr/lib/hadoop
export HADOOP_HOME=\$HADOOP
export HADOOP_BIN=\$HADOOP/bin
export HADOOP_LIB=\$HADOOP/lib
export HADOOP_LIBS=\`hadoop classpath | tr -d \\*\`
export HADOOP_CONF_DIR=/etc/hadoop/conf.deltarho
export HADOOP_COMMON_HOME=/usr/lib/hadoop
export HADOOP_MAPRED_HOME=/usr/lib/hadoop-mapreduce
export HADOOP_HDFS_HOME=/usr/lib/hadoop-hdfs
export YARN_HOME=/usr/lib/hadoop-yarn
export PATH=\$PATH:\$HADOOP_BIN&quot; |
sudo bash -c &quot;cat &gt; /etc/profile.d/hadoop.sh&quot;</code></pre>
</div>
<div id="r-all-nodes" class="section level3">
<h3>R (all nodes)</h3>
<p>(<a href="http://cran.r-project.org/" class="uri">http://cran.r-project.org/</a>)<br />R is installed and configured to know about Java. Install the Red Hat provided blas and lapack packages first for extra performance. Cd to a directory where the software can be downloaded and compiled and type:</p>
<pre><code>sudo yum install blas blas-devel lapack lapack-devel
wget http://cran.r-project.org/src/base/R-3/R-3.1.3.tar.gz
tar zxvf R-3.1.3.tar.gz
cd R-3.1.3
./configure --with-blas --with-lapack --with-x --enable-R-shlib
make
sudo make install
sudo env PATH=/usr/local/bin:$PATH R CMD javareconf</code></pre>
</div>
<div id="rjava-all-nodes" class="section level3">
<h3>rJava (all nodes)</h3>
<p>(<a href="http://www.rforge.net/rJava/" class="uri">http://www.rforge.net/rJava/</a>)<br />rJava is also required by RHIPE. It is installed by:</p>
<pre><code>sudo env PATH=/usr/local/bin:$PATH R -e &quot;install.packages(&#39;rJava&#39;, repos=&#39;http://cran.rstudio.com/&#39;)&quot;</code></pre>
</div>
<div id="rhipe-all-nodes" class="section level3">
<h3>RHIPE (all nodes)</h3>
<p>(<a href="http://deltarho.org/" class="uri">http://deltarho.org/</a>)<br />RHIPE is downloaded and installed.</p>
<pre><code>wget http://ml.stat.purdue.edu/rhipebin/Rhipe_0.75.1.4_cdh5.tar.gz
sudo env PATH=/usr/local/bin:$PATH R CMD INSTALL Rhipe_0.75.1.4_cdh5.tar.gz</code></pre>
</div>
<div id="datadr-and-trelliscope-support-packages-all-nodes" class="section level3">
<h3>datadr and trelliscope support packages (all nodes)</h3>
<p>(<a href="http://cran.r-project.org/web/packages/available_packages_by_name.html" class="uri">http://cran.r-project.org/web/packages/available_packages_by_name.html</a>)<br />Additional R packages - codetools, latttice, ggplot2, boot, shiny and devtools are needed for trelliscope and datadr. They are installed using the R internal package installer <code>install.packages</code>. Use the command line to enter the following commands.</p>
<pre><code>sudo yum install libcurl-devel
sudo env PATH=/usr/local/bin:$PATH R -e &quot;install.packages(&#39;ggplot2&#39;, repos=&#39;http://cran.rstudio.com/&#39;)&quot;
sudo env PATH=/usr/local/bin:$PATH R -e &quot;install.packages(&#39;lattice&#39;, repos=&#39;http://cran.rstudio.com/&#39;)&quot;
sudo env PATH=/usr/local/bin:$PATH R -e &quot;install.packages(&#39;RCurl&#39;, repos=&#39;http://cran.rstudio.com/&#39;)&quot;
sudo env PATH=/usr/local/bin:$PATH R -e &quot;install.packages(&#39;shiny&#39;, repos=&#39;http://cran.rstudio.com/&#39;)&quot;
sudo env PATH=/usr/local/bin:$PATH R -e &quot;install.packages(&#39;devtools&#39;, repos=&#39;http://cran.rstudio.com/&#39;)&quot;</code></pre>
</div>
<div id="datadr-and-trelliscope-all-nodes" class="section level3">
<h3>datadr and trelliscope (all nodes)</h3>
<p>(<a href="http://deltarho.org" class="uri">http://deltarho.org</a>)<br />datadr and trelliscope are installed using install_github package from R.</p>
<pre><code>sudo env PATH=/usr/local/bin:$PATH R -e &quot;options(repos = &#39;http://cran.rstudio.com/&#39;);\
 library(devtools); install_github(&#39;delta-rho/datadr&#39;)&quot;
sudo env PATH=/usr/local/bin:$PATH R -e &quot;options(repos = &#39;http://cran.rstudio.com/&#39;);\
 library(devtools); install_github(&#39;delta-rho/trelliscope&#39;)&quot;</code></pre>
</div>
<div id="shiny-server-frontend-server" class="section level3">
<h3>Shiny server (frontend server)</h3>
<p>(<a href="http://www.rstudio.com/products/shiny/shiny-server/" class="uri">http://www.rstudio.com/products/shiny/shiny-server/</a>)<br /> Shiny server is only installed on frontend.example.com, our designated Shiny server. The following commands will install the software and launch the Shiny server.</p>
<pre><code>wget http://download3.rstudio.org/centos-5.9/x86_64/shiny-server-1.3.0.403-rh5-x86_64.rpm
sudo yum install --nogpgcheck shiny-server-1.3.0.403-rh5-x86_64.rpm</code></pre>
</div>
<div id="rstudio-server-frontend-server" class="section level3">
<h3>Rstudio Server (frontend server)</h3>
<p>(<a href="http://www.rstudio.com/" class="uri">http://www.rstudio.com/</a>)<br />Rstudio Server is installed on just the shiny server, frontend.example.com. The following commands will install the software and launch Rstudio server.</p>
<pre><code>sudo yum install openssl098e
wget http://download2.rstudio.org/rstudio-server-0.98.1103-x86_64.rpm
sudo yum install --nogpgcheck rstudio-server-0.98.1103-x86_64.rpm</code></pre>
<p>Test Rstudio server by browsing to <a href="http://frontend.example.com:8787" class="uri">http://frontend.example.com:8787</a> and logging in as a normal user with accounts on that server. This may require firewall changes.</p>
</div>
</div>
<div id="configuration" class="section level2">
<h2>Configuration</h2>
<p>Hadoop’s HDFS will use space on the local disks on node002 through node005 to create a single large distributed filesystem. In our example we will say each of node002 through node005 has four 2TB drives per node, for a total of 32TB across the four servers. These local drives must be already been formatted by Linux as normal Linux filesystems (ext4, ext3, etc.). We will further say these filesystems have been mounted with the same mount point locations on all nodes: /hadoop/disk1, /hadoop/disk2, /hadoop/disk3, and /hadoop/disk4. HDFS actually permits different sized drives and different numbers of drives on each server, but doing so requires each server to have an individually maintained hdfs-site.xml file, described later. If possible, keep the number of drives the same on each node for easier administration. The names of the mount points aren’t special and could be anything, but these are the names we will use in this example. HDFS creates its own directory structure and files within the drives that are part of HDFS. If one were to look inside one of these filesystems after Hadoop is running and files have been written to HDFS, one would see a file and directory structure with computer generated directory names and filenames that don’t correspond in any way to their HDFS filenames (eg. blk_1073742354, blk_1073742354_1530.meta, etc.). Access to these files must be made through Hadoop. Paths to files in HDFS begin with ‘/’ just as they do on a normal Linux filesystem, but the namespace is completely independent from all other files on the nodes. This means the directory /tmp on HDFS is completely different from the normal /tmp visible at a bash prompt. Some paths in the configuration below are in the Linux filesystem namespace an some are in the HDFS filesystem namespace.</p>
<p>The frontend server does not directly participate in Hadoop computation, HDFS storage, etc. but it must have knowledge of the Hadoop configuration in order to interact with Hadoop. Thus all Hadoop configuration files must exist on the frontend in addition to the nodeNNN servers.</p>
<p>The amount of RAM per node must be known for some configuration settings, so lets say we have 64GB per node in our example configuration. We will say each node has 16 CPU cores.</p>
<div id="hosts-file-or-dns-entries" class="section level3">
<h3>Hosts File or DNS entries</h3>
<p>The hostnames and IP addresses of all machines in the Hadoop cluster must be added to the <code>/etc/hosts</code> file on each machine of the cluster, or be know to the DNS server each of the nodes uses. These are our example hostnames and IP addresses.</p>
<pre><code>10.0.0.001 node001.example.com node001
10.0.0.002 node002.example.com node002
10.0.0.003 node003.example.com node003
10.0.0.004 node004.example.com node004
10.0.0.005 node005.example.com node005
10.0.0.006 frontend.example.com frontend</code></pre>
</div>
<div id="firewalls" class="section level3">
<h3>Firewalls</h3>
<p>The new network services for Hadoop, including access to files in HDFS, will be visible to the Internet unless already blocked due to NAT or some other form of firewall elsewhere on the network. It is advised that a host-based firewall be erected using iptables on each of the nodes and the fronend server. It would permit full communication for all ports among the six servers in our example: node001 through node005 and the frontend server. It would restrict access to these servers from external hosts based on your local needs. For example, you may wish to permit ssh connections to the frontend from anywhere in the world, but allow ssh to the nodeNNN servers only from local subnets. You may want to permit Rstudio Server connections to the frontend (port 8787) from just three local subnets only, but permit no other network access to any other servers. Other ports you may wish to open to local subnets include port 50070 on node001 to see the status of HDFS, and port 8088 on node001 to see the status of the ResourceManager, the jobs that are running, have been completed, etc.</p>
</div>
<div id="hadoop-core-site.xml" class="section level3">
<h3>Hadoop core-site.xml</h3>
<p>The <code>core-site.xml</code> file is placed in <code>/etc/hadoop/conf.deltarho</code> on every server in the cluster (nodeNNN and frontend). If a change is ever made to this config file, it must be made to that file on every node in the cluster as well.</p>
<pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;
&lt;configuration&gt;
  &lt;property&gt;
    &lt;!-- This is the NameNode --&gt;
    &lt;name&gt;fs.defaultFS&lt;/name&gt;
    &lt;value&gt;hdfs://node001.example.com:8020&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;</code></pre>
</div>
<div id="hadoop-mapred-site.xml" class="section level3">
<h3>Hadoop mapred-site.xml</h3>
<p>The <code>mapred-site.xml</code> file in in <code>/etc/hadoop/conf.deltarho</code> is configured to tell Hadoop we are using the MRv2 / YARN framework. This file should be copied to all servers (nodeNNN and frontend).</p>
<pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;
&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
    &lt;value&gt;yarn&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;</code></pre>
</div>
<div id="hadoop-yarn-site.xml" class="section level3">
<h3>Hadoop yarn-site.xml</h3>
<p>The <code>yarn-site.xml</code> file in in <code>/etc/hadoop/conf.deltarho</code> is used to configure Hadoop settings related to temporary storage, number of CPU cores to use per node, memory per node, etc. This file should be copied to all servers (nodeNNN and frontend). Changes to this file can have significant performance impact, such as running 16 tasks per node rather than running 2 per node. Tuning these settings optimally is beyond the scope of this guide. In our example cluster, the hardware configuration of all nodes is identical, thus the same yarn-site.xml file can be copied around to all nodes. If in your configuration you have some nodes with different amounts of RAM or different numbers of CPU cores, they must have an individually maintained and updated <code>yarn-site.xml</code> file with those settings configured differently.</p>
<pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;
&lt;configuration&gt;
  &lt;property&gt;
    &lt;!-- This should match the name of the resource manager in your local deployment --&gt;
    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;
    &lt;value&gt;node001.example.com&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;!-- How much RAM on this server can be used for Hadoop --&gt;
    &lt;!-- We will use (total RAM - 2GB).  We have 64GB in our example, so use 62GB --&gt;
    &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;
    &lt;value&gt;62000&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;!-- How many CPU cores on this server can be used for Hadoop --&gt;
    &lt;!-- We will use them all, which is 16 per node in our example cluster --&gt;
    &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;
    &lt;value&gt;16&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;yarn.nodemanager.aux-services.mapreduce_shuffle.class&lt;/name&gt;
    &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt;
    &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;!-- List of directories to store temporary localized files. --&gt;
    &lt;!-- Spread these across all local drives on all nodes --&gt;
    &lt;name&gt;yarn.nodemanager.local-dirs&lt;/name&gt;
    &lt;value&gt;file:///hadoop/disk1/yarn/local,file:///hadoop/disk2/yarn/local,
           file:///hadoop/disk3/yarn/local,file:///hadoop/disk4/yarn/local&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;!-- Where to store temporary container logs. --&gt;
    &lt;!-- Spread these across all local drives on all nodes --&gt;
    &lt;name&gt;yarn.nodemanager.log-dirs&lt;/name&gt;
    &lt;value&gt;file:///hadoop/disk1/yarn/log,file:///hadoop/disk2/yarn/log,
           file:///hadoop/disk3/yarn/log,file:///hadoop/disk4/yarn/log&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;!-- This should match the name of the NameNode in your local deployment --&gt;
    &lt;name&gt;yarn.nodemanager.remote-app-log-dir&lt;/name&gt;
    &lt;value&gt;hdfs://node001.example.com/var/log/hadoop-yarn/apps&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;yarn.application.classpath&lt;/name&gt;
    &lt;value&gt;
       $HADOOP_CONF_DIR,
       $HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,
       $HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,
       $HADOOP_MAPRED_HOME/*,$HADOOP_MAPRED_HOME/lib/*,
       $HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*
    &lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;</code></pre>
<p>We reference new directories above in the local Linux filesystem into which logs and other files are placed. We must create these directories now and set the proper owner, group, and permissions. This must be done on node001 - node005:</p>
<pre><code>sudo mkdir -p /hadoop/disk1/yarn/local /hadoop/disk2/yarn/local
sudo mkdir -p /hadoop/disk3/yarn/local /hadoop/disk4/yarn/local
sudo mkdir -p /hadoop/disk1/yarn/log /hadoop/disk2/yarn/log
sudo mkdir -p /hadoop/disk3/yarn/log /hadoop/disk4/yarn/log
sudo chown -R yarn.yarn /hadoop/disk1/yarn/local /hadoop/disk2/yarn/local
sudo chown -R yarn.yarn /hadoop/disk3/yarn/local /hadoop/disk4/yarn/local
sudo chown -R yarn.yarn /hadoop/disk1/yarn/log /hadoop/disk2/yarn/log
sudo chown -R yarn.yarn /hadoop/disk3/yarn/log /hadoop/disk4/yarn/log</code></pre>
</div>
<div id="hadoop-secondary-namenode-file-masters" class="section level3">
<h3>Hadoop secondary namenode file masters</h3>
<p>The file <code>/etc/hadoop/conf.deltarho/masters</code> must contain the hostname of the secondary namenode. This file should be copied to all servers. For our example, it contains just one line:</p>
<pre><code>node002.example.com</code></pre>
</div>
<div id="hadoop-hdfs-site.xml" class="section level3">
<h3>Hadoop hdfs-site.xml</h3>
<p>The <code>hdfs-site.xml</code> file is used to configure the Hadoop file system (HDFS). This file should be copied to the <code>/etc/hadoop/conf.deltarho</code> directory on all nodes (nodeNNN and frontend). As with <code>yarn-site.xml</code>, if your number of disks and size of disk varies among nodes, you must independently maintain copies of <code>hdfs-site.xml</code> on each node.</p>
<pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;
&lt;configuration&gt;
  &lt;property&gt;
    &lt;!--  Number of times each HDFS block is replicated.  Default is 3. --&gt;
    &lt;name&gt;dfs.replication&lt;/name&gt;
    &lt;value&gt;3&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;!-- Size in bytes of each HDFS block.  Should be a power of 2. --&gt;
    &lt;!--  We use 2^27 --&gt;
    &lt;name&gt;dfs.blocksize&lt;/name&gt;
    &lt;value&gt;134217728&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- Where the namenode stores HDFS metadata on its local drives --&gt;
  &lt;!-- These are Linux filesystem paths that must already exist. --&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
    &lt;value&gt;file:///hadoop/disk1/dfs/nn,file:///hadoop/disk2/dfs/nn,
           file:///hadoop/disk3/dfs/nn,file:///hadoop/disk4/dfs/nn&lt;/value&gt;
  &lt;/property&gt;

  &lt;!-- Where the secondary namenode stores HDFS metadata on its local drives --&gt;
  &lt;!-- These are Linux filesystem paths that must already exist. --&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt;
    &lt;value&gt;file:///hadoop/disk1/dfs/nn,file:///hadoop/disk2/dfs/nn,
           file:///hadoop/disk3/dfs/nn,file:///hadoop/disk4/dfs/nn&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;!-- Where each datanode stores HDFS blocks on its local drives. --&gt;
    &lt;!-- These are Linux filesystem paths that must already exist. --&gt;
    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
    &lt;value&gt;file:///hadoop/disk1/dfs/dn,file:///hadoop/disk2/dfs/dn,
           file:///hadoop/disk3/dfs/dn,file:///hadoop/disk4/dfs/dn&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;!-- This should match the name of the NameNode in your local deployment --&gt;
    &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;
    &lt;value&gt;node001.example.com:50070&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
     &lt;name&gt;dfs.permissions.superusergroup&lt;/name&gt;
     &lt;value&gt;hadoop&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.client.read.shortcircuit.streams.cache.size&lt;/name&gt;
    &lt;value&gt;1000&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.client.read.shortcircuit.streams.cache.expiry.ms&lt;/name&gt;
    &lt;value&gt;10000&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;!-- Leave the dn._PORT as is, do not try to make this a number --&gt;
    &lt;name&gt;dfs.domain.socket.path&lt;/name&gt;
    &lt;value&gt;/var/run/hadoop-hdfs/dn._PORT&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;</code></pre>
<p>We must pre-create the local metadata storage directories on the NameNode (node001), and on the Secondary NameNode (node002):</p>
<pre><code>sudo mkdir -p /hadoop/disk1/dfs/nn /hadoop/disk2/dfs/nn
sudo mkdir -p /hadoop/disk3/dfs/nn /hadoop/disk4/dfs/nn
sudo chown -R hdfs.hdfs /hadoop/disk1/dfs /hadoop/disk2/dfs
sudo chown -R hdfs.hdfs /hadoop/disk3/dfs /hadoop/disk4/dfs
sudo chmod 700 /hadoop/disk1/dfs /hadoop/disk2/dfs /hadoop/disk3/dfs /hadoop/disk4/dfs</code></pre>
<p>We must pre-create the local storage directories on the datanodes, node002 through node005:</p>
<pre><code>sudo mkdir -p /hadoop/disk1/dfs/dn /hadoop/disk2/dfs/dn
sudo mkdir -p /hadoop/disk3/dfs/dn /hadoop/disk4/dfs/dn
sudo chown -R hdfs.hdfs /hadoop/disk1/dfs /hadoop/disk2/dfs
sudo chown -R hdfs.hdfs /hadoop/disk3/dfs /hadoop/disk4/dfs
sudo chmod 700 /hadoop/disk1/dfs /hadoop/disk2/dfs
sudo chmod 700 /hadoop/disk3/dfs /hadoop/disk4/dfs</code></pre>
</div>
</div>
<div id="hadoop-pre-run-setup" class="section level2">
<h2>Hadoop Pre-run Setup</h2>
<p>At this point the configuration files created in <code>/etc/hadoop/conf.deltarho</code> on node001 should be copied to all other nodes. Any future changes to any configuration files should be done on node001 and then copied from there to all other nodes.</p>
<p>Note: In the event that some nodes have different number of drives, or the paths to those drives differ, or they have a different number of CPUs, different amounts of RAM, etc. then separate, independent <code>hdfs-site.xml</code> and <code>yarn-site.xml</code> files may be necessary on each node.</p>
<p>Next we format the new HDFS filesystem before actually starting the NameNode service for the first time. This is done on the NameNode (node001) only. It must be done as the user “hdfs”:</p>
<pre><code>sudo -u hdfs hdfs namenode -format</code></pre>
<p>Manually start hdfs first by starting the NameNode service on node001:</p>
<pre><code>sudo service hadoop-hdfs-namenode start</code></pre>
<p>Then start the datanode service on node002 - node005 by typing this on each of them one at a time:</p>
<pre><code>sudo service hadoop-hdfs-datanode start</code></pre>
<p>Once the NameNode service is running on node001 and the DataNode service is running on the other nodes, it is time to create new folders in HDFS for Hadoop to use and set permissions correctly. Note these are HDFS paths, NOT normal Linux filesystem paths:</p>
<pre><code>sudo -u hdfs hadoop fs -mkdir -p /tmp/hadoop-yarn/staging
sudo -u hdfs hadoop fs -chmod -R 1777 /tmp
sudo -u hdfs hadoop fs -mkdir -p /user/history
sudo -u hdfs hadoop fs -chmod -R 1777 /user/history
sudo -u hdfs hadoop fs -chown mapred:hadoop /user/history
sudo -u hdfs hadoop fs -mkdir -p /var/log/hadoop-yarn
sudo -u hdfs hadoop fs -chown yarn:mapred /var/log/hadoop-yarn</code></pre>
<p>The administrator much make some choices about where users will be storing their files, what permissions should exist on the /user directory, etc. If privacy is important, the administrator must create user directories in HDFS individually for every user on the system and set the permissions on those directories accordingly. For example, to create a private storage location for users joe and bob, the adminsitrator would type:</p>
<pre><code>sudo -u hdfs hadoop fs -mkdir /user/joe /user/bob
sudo -u hdfs hadoop fs -chown joe /user/joe
sudo -u hdfs hadoop fs -chown bob /user/bob
sudo -u hdfs hadoop fs -chmod  700 /user/joe /user/bob</code></pre>
<p>This would permit joe and only joe to read, write, and create files in <code>/user/joe</code>. Only bob could read, write, and create files in <code>/user/bob</code>.</p>
<p>In a research group where all users are permitted to see files created by all other users, the easiest approach is just to set permissions such that anyone can create new directories inside the /user directory themselves. To do this, the administrator would set the permissions like this:</p>
<pre><code>sudo -u hdfs hadoop fs -chmod 1777 /user</code></pre>
<p>Then an individual user, like bob, could create his own directory where he can store data, rather than having the adminsitrator create directories for everyone individually. Bob could just log into the frontend server and type this himself:</p>
<pre><code>hadoop fs -mkdir /user/bob</code></pre>
</div>
<div id="starting-the-hadoop-cluster" class="section level2">
<h2>Starting the Hadoop Cluster</h2>
<p>Configuration settings and environment varaibles have been changed such that rebooting all servers should cause all Hadoop services to automatically start and the environment to be correctly set for all new logins. Do that now by rebooting node001 through node005, and the frontend server, by typing this on each node:</p>
<pre><code>sudo reboot</code></pre>
</div>
<div id="checking-the-status-of-the-hadoop-cluster" class="section level2">
<h2>Checking the status of the Hadoop Cluster</h2>
<p>You can see the status of the ResourceManager, the jobs that are running, have been completed, etc. by browsing to <a href="http://node001.example.com:8088" class="uri">http://node001.example.com:8088</a>. This may require firewall settings to be adjusted before this is visible to your browser. Clicking on the Nodes link in the left sidebar should show that node002 through node005 are running and reporting in.</p>
<p>To see the status of HDFS, which nodes are up, how much space is used and available, etc., browse to <a href="http://node001.example.com:50070" class="uri">http://node001.example.com:50070</a>. This may also require firewall settings to be adjusted before this is visible to your browser. Clicking the Datanodes link should show that node002 through node005 are running and reoprting in.</p>
</div>
<div id="notes" class="section level2">
<h2>Notes</h2>
<div id="raid-and-redundancy-design-under-hadoophdfs" class="section level3">
<h3>RAID and Redundancy Design under Hadoop/HDFS</h3>
<ul>
<li><p>RAID configurations are usually not recommended for HDFS data drives. HDFS already handles fault tolerance by distributing the blocks it writes to local drives among all nodes for both performance and redundancy. RAID won’t improve performance, and could even slow things down. In some configurations it will reduce overall HDFS capacity.</p></li>
<li><p>The default block redundancy setting for HDFS is three replicates, as specified by the <code>dfs.replication</code> variable in <code>hdfs-site.xml</code>. This means that each data block is copied to three drives, optimally on three different nodes. Hadoop has shown high availability is possible with three replicates. The downside is the total capacity of HDFS is divided by the number of replicates used. This means our 32 TB example cluster with three replicates can only hold 10.67 TB of data. Decreasing <code>dfs.replication</code> below 3 is not recommended and should be avoided. Increasing it above 3 could increase performance for large clusters under certain workloads, but at the cost of capacity.</p></li>
</ul>
</div>
<div id="r-package-design" class="section level3">
<h3>R Package Design</h3>
<ul>
<li><p>Most R packages can be completely provided by the system administrator by installing them as root, which implicitly places them in a system wide accessible location, for example <code>/usr/lib/R/library</code> or <code>/usr/local/lib64/R/library</code>.</p></li>
<li><p>Alternately, the system administrator can install just the core default R packages at a system wide location and allow individual users to install specific R library packages in their home directory. This permits users the flexibility to easily change versions of packages they are using and update them when they choose.</p></li>
</ul>
</div>
<div id="rhipe_hadoop_tmp_folder-environment-variable" class="section level3">
<h3>RHIPE_HADOOP_TMP_FOLDER environment variable</h3>
<ul>
<li><p>It has been observed that some versions of Linux, such as Red Hat Enterprise Linux, may have an issue with RHIPE where it will give false errors about being unable to write files in HDFS, even where the directory in question is clearly writable. This can be corrected by creating a directory somewhere in HDFS that is readable only by that user, and then setting the RHIPE_HADOOP_TMP_FOLDER environment variable to point to that HDFS directory. The user bob for example, would type this on frontend:</p>
<pre><code>hadoop fs -mkdir -p /user/bob/tmp
hadoop fs -chmod 700 /user/bob/tmp</code></pre>
<p>He would then add this to his environment by including it in his .bashrc or .bash_profile, or whatever location is appropriate for his shell:</p>
<pre><code>export RHIPE_HADOOP_TMP_FOLDER=/user/bob/tmp</code></pre></li>
</ul>
</div>
<div id="consider-using-hdfs-high-availability-rather-than-a-secondary-namenode" class="section level3">
<h3>Consider using HDFS High Availability rather than a Secondary NameNode</h3>
<p>The primary role of the Secondary NameNode is to perform periodic checkpointing so the NameNode doesn’t have to, which makes reboots of the cluster go much more quickly. The Secondary NameNode could also be used to reconstruct the majority of HDFS if the NameNode were to have a catastrophic failure, but through a manual, imperfect process prone to error. For a more robust, fault tolerant Hadoop configuration, consider using a High Availability HDFS configuration, which uses a Standby NameNode rather than a Secondary NameNode, and can be configured for automatic failover in the event of a NameNode failure. This configuration is more complex, requires the use of Zookeeper, three or more JournalNode hosts (these can be regular nodes), and another node dedicated to act as the Standby NameNode. The documentation at cloudera.com describes this in more detail.</p>
</div>
</div>
</div>
<div id="installing-r-packages-and-examples-for-users" class="section level1">
<h1>Installing R Packages and Examples for Users</h1>
<div id="background" class="section level2">
<h2>Background</h2>
<p>At this point, the DeltaRho stack, including Hadoop (a YARN based distribution, such as Cloudera CDH5.x), R, and RHIPE should be installed. However, at some sites the administrator may choose to only maintain Hadoop, Java, Protocol Buffers, and R and rely on the end users to install the various R packages, like RHIPE and rJava to their own personal accounts.</p>
<p>Ideally, the versions of Linux R, and Java, are the same on both the front-end R session server and the Hadoop cluster nodes. Java in particular plays a critical roll in RHIPE, and Java likes homogeneity.</p>
</div>
<div id="install-and-push" class="section level2">
<h2>Install and Push</h2>
<p>If the administrator has not already installed RHIPE, you first first download the package file by typing:</p>
<pre><code>wget http://ml.stat.purdue.edu/rhipebin/Rhipe_0.75.1.4_cdh5.tar.gz</code></pre>
<p>Then launch R and type the following to install rJava and RHIPE:</p>
<pre><code>install.packages(&quot;rJava&quot;)
install.packages(&quot;Rhipe_0.75.1.4_cdh5.tar.gz&quot;, repos=NULL, type=&quot;source&quot;)</code></pre>
<p>RHIPE is now installed. Each time you start an R session and you want RHIPE to be available, type:</p>
<pre><code>library(Rhipe)
rhinit()</code></pre>
<p>As a one-time configuration step, you push all the R packages you have installed on the R session server, including RHIPE, onto the cluster HDFS. First, you need the system administrator to configure a directory in HDFS that is writable by you. We will assume the administrator has created for you the writable directory <code>/user/loginname</code> using your login name, and has done the same thing for other users. Suppose in <code>/user/loginname</code> you want to create a directory <code>bin</code> on HDFS where you will push your installations on the R session server. You can do this and carry out the push by</p>
<pre><code>rhmkdir(&quot;/user/loginname/bin&quot;)
hdfs.setwd(&quot;/user/loginname/bin&quot;)
bashRhipeArchive(&quot;R.Pkg&quot;)</code></pre>
<p><code>rhmkdir()</code> creates your directory <code>bin</code> in the directory <code>/user/loginname</code>. <code>hdfs.setwd()</code> declares <code>/user/loginname/bin</code> to be the directory with your choice of installations. <code>bashRhipeArchive()</code> creates the actual archive of your installations and names it as <code>R.Pkg</code>.</p>
<p>Each time your R code will require the installations on the HDFS, you must in your R session run</p>
<pre><code>library(Rhipe) rhinit()
rhoptions(zips = &quot;/user/loginname/bin/R.Pkg.tar.gz&quot;)
rhoptions(runner = &quot;sh ./R.Pkg/library/Rhipe/bin/RhipeMapReduce.sh&quot;)</code></pre>
</div>
<div id="example-housing-data" class="section level2">
<h2>Example: Housing Data</h2>
<div id="the-data" class="section level3">
<h3>The Data</h3>
<p>The housing data consist of 7 monthly variables on housing sales from Oct 2008 to Mar 2014, which is 66 months. The measurements are for 2883 counties in 48 U.S. states, excluding Hawaii and Alaska, and also for the District of Columbia which we treat as a state with one county. The data were derived from sales of housing units from Quandl’s Zillow Housing Data (www.quandl.com/c/housing). A housing unit is a house, an apartment, a mobile home, a group of rooms, or a single room that is occupied or intended to be occupied as a separate living quarter.</p>
<p>The variables are</p>
<ul>
<li><p><code>FIPS</code>: FIPS county code, an unique identifier for each U.S. county</p></li>
<li><p><code>county</code>: county name</p></li>
<li><p><code>state</code>: state abbreviation</p></li>
<li><p><code>date</code>: time of sale measured in months, from 1 to 66</p></li>
<li><p><code>units</code>: number of units sold</p></li>
<li><p><code>listing</code>: monthly median listing price (dollars per square foot)</p></li>
<li><p><code>selling</code>: monthly median selling price (dollars per square foot)</p></li>
</ul>
<p>Many observations of the last three variables are missing: units 68%, listing 7%, and selling 68%.</p>
<p>The number of measurements (including missing), is 7 x 66 x 2883 = 1,331,946. So this is in fact a small dataset that could be analyzed in the standard serial R. However, we can use them to illustrate how RHIPE R commands implement Divide and Recombine. We simply pretend the data are large and complex, break into subsets, and continuing on with D&amp;R. The small size let’s you easily pick up the data, follow along using the R commands in the tutorial, and explore RHIPE yourself with other RHIPE R commands.</p>
<p>“housing.txt” is available in our DeltaRhodata Github repository of the RHIPE documentation, or at:</p>
<pre><code>https://raw.githubusercontent.com/delta-rho/docs-RHIPE/gh-pages/housing.txt</code></pre>
<p>The file is a table with 190,278 rows (66 months x 2883 counties) and 7 columns (the variables). The fields in each row are separated by a comma, and there are no headers in the first line. Here are the first few lines of the file:</p>
<pre><code>01001,Autauga,AL,1,27,96.616541353383,99.1324
01001,Autauga,AL,2,28,96.856993190152,95.8209
01001,Autauga,AL,3,16,98.055555555556,96.3528
01001,Autauga,AL,4,23,97.747480735033,95.2189
01001,Autauga,AL,5,22,97.747480735033,92.7127</code></pre>
</div>
<div id="write-housing.txt-to-the-hdfs" class="section level3">
<h3>Write housing.txt to the HDFS</h3>
<p>To get started, we need to make <code>housing.txt</code> available as a text file within the HDFS file system. This puts it in a place where it can be read into R, form subsets, and write the subsets to the HDFS. This is similar to what we do using R in the standard serial way; if we have a text file to read into R, we move put it in a place where we can read it into R, for example, in the working directory of the R session.</p>
<p>To set this up, the system administrator must do two tasks. On the R session server, set up a home directory where you have write permission; let’s call it <code>/home/loginname</code>. In the HDFS, the administrator does a similar thing, creates, say, <code>/user/loginname</code> which is in the root directory.</p>
<p>Your first step, as for the standard R case, is to copy <code>housing.txt</code> to a directory on the R-session server where your R session is running. Suppose in your login directory you have created a directory <code>housing</code> for your analysis of the housing data. You can copy <code>housing.txt</code> to</p>
<pre><code>&quot;/home/loginname/housing/&quot;</code></pre>
<p>The next step is to get <code>housing.txt</code> onto the HDFS as a text file, so we can read it into R on the cluster. There are Hadoop commands that could be used directly to copy the file, but our promise to you is that you never need to use Hadoop commands. There is a RHIPE function, <code>rhput()</code> that will do it for you.</p>
<pre><code>rhput(&quot;/home/loginname/housing/housing.txt&quot;, &quot;/user/loginname/housing/housing.txt&quot;)</code></pre>
<p>The <code>rhput()</code> function takes two arguments. The first is the path name of the R server file to be copied. The second argument is the path name HDFS where the file will be written. Note that for the HDFS, in the directory <code>/user/loginname</code> there is a directory <code>housing</code>. You might have created <code>housing</code> already with the command</p>
<pre><code>rhmkdir(&quot;/user/loginname/housing&quot;)</code></pre>
<p>If not, then <code>rhput()</code> creates the directory for you.</p>
<p>We can confirm that the housing data text file has been written to the HDFS with the <code>rhexists()</code> function.</p>
<pre><code>rhexists(&quot;/user/loginname/housing/housing.txt&quot;)
[1] TRUE</code></pre>
<p>We can use <code>rhls()</code> to get more information about files on the HDFS. It is similar to the Unix command <code>ls</code>. For example,</p>
<pre><code>rhls(&quot;/user/loginname/housing&quot;)
  permission         owner      group     size          modtime
1 -rw-rw-rw- loginname supergroup 7.683 mb 2014-09-17 11:11
                              file
/user/loginname/housing/housing.txt</code></pre>
</div>
<div id="read-and-divide-by-county" class="section level3">
<h3>Read and Divide by County</h3>
<p>Our division method for the housing data will be to divide by county, so there will be 2883 subsets. Each subset will be a <code>data.frame</code> object with 4 column variables: <code>date</code>, <code>units</code>, <code>listing</code>, and <code>selling</code>. <code>FIPS</code>, <code>state</code>, and <code>county</code> are not column variables because each has only one value for each county; their values are added to the <code>data.frame</code> as attributes.</p>
<p>The first step is to read each line of the file <code>housing.txt</code> into R. By convention, RHIPE takes each line of a text file to be a key-value pair. The line number is the key. The value is the data for the line, in our case the 7 observations of the 7 variables of the data for one month and one county.</p>
<p>Each line is read as part of Map R code written by the user. The Map input key-value pairs are the above line key-value pairs. Each line also has a Map output key-value pair. The key identifies the county. <code>FIPS</code> could have been enough to do this, but it is specified as a character vector with three elements: the 3-vector values of <code>FIPS</code>, <code>state</code>, and <code>county</code>. This is done so that later all three can be added to the subset <code>data.frame</code>. The output value for each output key is the observations of <code>date</code>, <code>units</code>, <code>listing</code>, and <code>selling</code> from the line for that key.</p>
<p>The Map output key-value pairs are the input key-value pairs for the Reduce R code written by the user. Reduce assembles these into groups by key, that is, the county. Then the Reduce R code is applied to the output values of each group collectively to create the subset <code>data.frame</code> object for each county. Each row is the value of one Reduce input key-value pair: observations of <code>date</code>, <code>units</code>, <code>listing</code>, and <code>selling</code> for one housing unit. <code>FIPS</code>, <code>state</code>, and <code>county</code> are added to the <code>data.frame</code> as attributes. Finally, Reduce writes each subset <code>data.frame</code> object to a directory in the HDFS specified by the user. The subsets are written as Reduce output key-value pairs. The output keys are the the values of <code>FIPS</code>. The output values are the county <code>data.frame</code> objects.</p>
<p><strong>The RHIPE Manager: rhwatch()</strong></p>
<p>We begin with the RHIPE R function <code>rhwatch()</code>. It runs the R code you write to specify Map and Reduce operations, takes your specification of input and output files, and manages key-value pairs for you.</p>
<p>The code for the county division is</p>
<pre><code>mr1 &lt;- rhwatch(
  map      = map1,
  reduce   = reduce1,
  input    = rhfmt(&quot;/user/loginname/housing/housing.txt&quot;, type = &quot;text&quot;),
  output   = rhfmt(&quot;/user/loginname/housing/byCounty&quot;, type = &quot;sequence&quot;),
  readback = FALSE
)</code></pre>
<p>Arguments <code>map</code> and <code>reduce</code> take your Map and Reduce R code, which will be described below. <code>input</code> specifies the input to be the text file in the HDFS that we put there earlier using <code>rhput()</code>. The file supplies input key-value pairs for the Map code. <code>output</code> specifies the file name into which final output key-value pairs of the Reduce code that are written to the HDFS. <code>rhwatch()</code> creates this file if it does not exist, or overwrites it if it does not.</p>
<p>In our division by county here, the Reduce recombination outputs are the 2883 county <code>data.frame</code> R objects. They are a <code>list</code> object that describes the key-value pairs: <code>FIPS</code> key and <code>data.frame</code> value. There is one <code>list</code> element per pair; that element is itself a list with two elements, the <code>FIPS</code> key and then the <code>data.frame</code> value.</p>
<p>The Reduce <code>list</code> output can also be written to the R global environment of the R session. One use of this is analytic recombination in the R session when the outputs are a small enough dataset. You can do this with the argument <code>readback</code>. If <code>TRUE</code>, the list is also written to the global environment. If <code>FALSE</code>, it is not. If FALSE, it can be written latter using the RHIPE R function <code>rhread()</code>.</p>
<pre><code>countySubsets &lt;- rhread(&quot;/user/loginname/housing/byCounty&quot;)</code></pre>
<p>Suppose you just want to look over the <code>byCounty</code> file on the HDFS just to see if all is well, but that this can be done by looking at a small number of key-value pairs, say 10. The code for this is</p>
<pre><code>countySubsets &lt;- rhread(&quot;/user/loginname/housing/byCounty&quot;, max = 10)
Read 10 objects(31.39 KB) in 0.04 seconds</code></pre>
<p>Then you can look at the list of length 10 in various was such as</p>
<pre><code>keys &lt;- unlist(lapply(countySubsets, &quot;[[&quot;, 1))
keys
[1] &quot;01013&quot; &quot;01031&quot; &quot;01059&quot; &quot;01077&quot; &quot;01095&quot; &quot;01103&quot; &quot;01121&quot; &quot;04001&quot; &quot;05019&quot; &quot;05037&quot;

attributes(countySubsets[[1]][[2]])
$names
[1] &quot;date&quot;             &quot;units&quot;            &quot;listing&quot;             &quot;selling&quot;
$row.names
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24
[25] 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48
[49] 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66
$state
[1] &quot;AL&quot;
$FIPS
[1] &quot;01013&quot;
$county
[1] &quot;Butler&quot;
$class
[1] &quot;data.frame&quot;</code></pre>
<p><strong>Map R Code</strong> The Map R code for the county division is</p>
<pre><code>map1 &lt;- expression({
  lapply(seq_along(map.keys), function(r) {
    line = strsplit(map.values[[r]], &quot;,&quot;)[[1]]
    outputkey &lt;- line[1:3]
    outputvalue &lt;- data.frame(
      date = as.numeric(line[4]),
      units =  as.numeric(line[5]),
      listing = as.numeric(line[6]),
      selling = as.numeric(line[7]),
      stringsAsFactors = FALSE
    )
  rhcollect(outputkey, outputvalue)
  })
})</code></pre>
<p>Map has input key-value pairs, and output key-value pairs. Each pair has an identifier, the key, and numeric-categorical information, the value. The Map R code is applied to each input key-value pair, producing one output key-value pair. Each application of the Map code to a key-value pair is carried out by a mapper, and there are many mappers running in parallel without communication (embarrassingly parallel) until the Map job completes.</p>
<p>RHIPE creates input key-value pair <code>list</code> objects, <code>map.keys</code> and <code>map.values</code>, based on information that it has. Let <code>r</code> be an integer from 1 to the number of input key-value pairs. <code>map.values[[r]]</code> is the value for key <code>map.keys[[r]]</code>. The housing data inputs come from a text file in the HDFS, housing.txt, By RHIPE convention, for a text file, each Map input key is a text file line number, and the corresponding Map input value is the observations in the line, read into R as a single text string. In our case each line value is the observations of the 7 county variables for the line.</p>
<p>This Map code is really a “for loop” with <code>r</code> as the looping variable, but is done by <code>lapply()</code> because it is in general faster than <code>for r in 1:length(map.keys)</code>. The loop proceeds through the input keys, specified by the first argument of <code>lapply</code>. The second argument of the above <code>lapply</code> defines the Map expression with the argument <code>r</code>, an index for the Map keys and values.</p>
<p>The function <code>strsplit()</code> splits each character-string line input value into the individual observations of the text line. The result, <code>line</code>, is a <code>list</code> of length one whose element is a <code>character vector</code> whose elements are the line observations. In our case, the observations are a <code>character vector</code> of length 7, in order: <code>FIPS</code>, <code>county</code>, <code>state</code>, <code>date</code>, <code>units</code>, <code>listing</code>, <code>selling</code>.</p>
<p>Next we turn to the Map output key-value pairs. <code>outputkey</code> for each text line is a character vector of length 3 with <code>FIPS</code>, <code>county</code>, and <code>state</code>. <code>utputvalue</code> is a <code>data.frame</code> with one row and 4 columns, the observations of <code>date</code>, <code>units</code>, <code>listing</code>, and <code>selling</code>, each a <code>numeric</code> object.</p>
<p>The argument of <code>data.frame</code>, <code>stringsAsFactors</code>, is is given the value <code>FALSE</code>. This leaves character vectors in the <code>data.frame</code> as is, and does on convert to a <code>factor</code>.</p>
<p>The RHIPE function <code>rhcollect()</code> forms a Map output key-value pair for each line, and writes the results to the HDFS as a key-value pair <code>list</code> object.</p>
<p><strong>Reduce R Code</strong> The Reduce R code for the county division is</p>
<pre><code>reduce1 &lt;- expression(
  pre = {
    reduceoutputvalue &lt;- data.frame()
  },
  reduce = {
    reduceoutputvalue &lt;- rbind(reduceoutputvalue, do.call(rbind, reduce.values))
  },
  post = {
    reduceoutputkey &lt;- reduce.key[1]
    attr(reduceoutputvalue, &quot;location&quot;) &lt;- reduce.key[1:3]
    names(attr(reduceoutputvalue, &quot;location&quot;)) &lt;- c(&quot;FIPS&quot;,&quot;county&quot;,&quot;state&quot;)
    rhcollect(reduceoutputkey, reduceoutputvalue)
  }
)</code></pre>
<p>The output key-value pairs of Map are the input key-value pairs to Reduce. The first task of Reduce is to group its input key-value pairs by unique key. The Reduce R code is applied to the key-value pairs of each group by a reducer. The number of groups varies in applications from just one, with a single Reduce output, to many. For multiple groups, the reducers run in parallel, without communication, until the Reduce job completes.</p>
<p>RHIPE creates two list objects <code>reduce.key</code> and <code>reduce.values</code>. Each element of <code>reduce.key</code> is the key for one group, and the corresponding element of <code>reduce.values</code> has the values for the group to which the Reduce code is applied. Now in our case, the key is county and the values are the observations of <code>date</code>, <code>units</code>, <code>listing</code>, and <code>selling</code> for the all housing units in the county.</p>
<p>Note the Reduce code has a certain structure: expressions <code>pre</code>, <code>reduce</code>, and <code>post</code>. In our case <code>pre</code> initializes <code>reduceoutputvalue</code> to a <code>data.frame()</code>. <code>reduce</code> assembles the county <code>data.frame</code> as the reducer receives the values through <code>rbind(reduceoutputvalue, do.call(rbind, reduce.values))</code>; this uses <code>rbind()</code> to add rows to the <code>data.frame</code> object. <code>post</code> operates further on the result of <code>reduce</code>. In our case it first assigns the observation of <code>FIPS</code> as the key. Then it adds <code>FIPS</code>,<code>county</code>, and <code>state</code> as <code>attributes</code>. Finally the RHIPE function <code>rhcollect()</code> forms a Reduce output key-value pair <code>list</code>, and writes it to the HDFS.</p>
</div>
<div id="compute-county-min-median-max" class="section level3">
<h3>Compute County Min, Median, Max</h3>
<p>With the county division subsets now in the HDFS we will illustrate using them to carry out D&amp;R with a very simple recombination procedure based on a summary statistic for each county of the variable <code>listing</code>. We do this for simplicity of explanation of how RHIPE works. However, we emphasize that in practice, initial analysis would almost always involve comprehensive analysis of both the detailed data for all subset variables and summary statistics based on the detailed data.</p>
<p>Our summary statistic consists of the minimum, median, and maximum of <code>listing</code>, one summary for each county. Map R code computes the statistic. The output key of Map, and therefore the input key for Reduce is <code>state</code>. The Reduce R code creates a <code>data.frame</code> for each state where the columns are <code>FIPS</code>, <code>county</code>, <code>min</code>, <code>median</code>, and <code>max</code>. So our example illustrates a scenario where we create summary statistics, and then analyze the results. This is an analytic recombination. In addition, we suppose that in this scenario the summary statistic dataset is small enough to analyze in the standard serial R. This is not uncommon in practice even when the raw data are very large and complex.</p>
<p><strong>The RHIPE Manager: rhwatch()</strong></p>
<p>Here is the code for <code>rhwatch()</code>.</p>
<pre><code>CountyStats &lt;- rhwatch(
  map      = map2,
  reduce   = reduce2,
  input    = rhfmt(&quot;/user/loginname/housing/byCounty&quot;, type = &quot;sequence&quot;),
  output   = rhfmt(&quot;/user/loginname/housing/CountyStats&quot;, type = &quot;sequence&quot;),
  readback = TRUE
)</code></pre>
<p>Our Map and Reduce code, <code>map2</code> and <code>reduce2</code>, is given to the arguments <code>map</code> and <code>reduce</code>. The code will be will be discussed later.</p>
<p>The input key-value pairs for Map, given to the argument <code>input</code>, are our county subsets which were written to the HDFS directory <code>/user/loginname/housing</code> as the key-value pairs <code>list</code> object <code>byCounty</code>. The final output key-value pairs for Reduce, specified by the argument <code>output</code>, will be written to the <code>list</code> object <code>CountyStats</code> in the same directory as the subsets. The keys are the states, and the values are the <code>data.frame</code> objects for the states.</p>
<p>The argument <code>readback</code> is given the value TRUE, which means <code>CountyStats</code> is also written to the R global environment of the R session. We do this because our scenario is that analytic recombination is done in R.</p>
<p>The argument <code>mapred.reduce.tasks</code> is given the value 10, as in our use of it to create the county subsets.</p>
<p><strong>The Map R Code</strong></p>
<p>The Map R code is</p>
<pre><code>map2 &lt;- expression({
  lapply(seq_along(map.keys), function(r) {
    outputvalue &lt;- data.frame(
      FIPS = map.keys[[r]],
      county = attr(map.values[[r]], &quot;location&quot;)[&quot;county&quot;],
      min = min(map.values[[r]]$listing, na.rm = TRUE),
      median = median(map.values[[r]]$listing, na.rm = TRUE),
      max = max(map.values[[r]]$listing, na.rm = TRUE),
      stringsAsFactors = FALSE
    )
    outputkey &lt;- attr(map.values[[r]], &quot;location&quot;)[&quot;state&quot;]
    rhcollect(outputkey, outputvalue)
  })
})</code></pre>
<p><code>map.keys</code> is the Map input keys, the county subset identifiers <code>FIPS</code>. <code>map.values</code> is the Map input values, the county subset <code>data.frame</code> objects. The <code>lapply()</code> loop goes through all subsets, and the looping variable is <code>r</code>. Each stage of the loop creates one output key-value pair, <code>outputkey</code> and <code>outputvalue</code>. <code>outputkey</code> is the observation of <code>state</code>. <code>outputvalue</code> is a <code>data.frame</code> with one row that has the variables <code>FIPS</code>, <code>county</code>, <code>min</code>, <code>median</code>, and <code>max</code> for county <code>FIPS</code>. <code>rhcollect(outputkey, outputvalue)</code> emits the pairs to reducers, becoming the Reduce input key-value pairs.</p>
<p><strong>The Reduce R Code</strong> The Reduce R code for the <code>listing</code> summary statistic is</p>
<pre><code>reduce2 &lt;- expression(
  pre = {
    reduceoutputvalue &lt;- data.frame()
  },
  reduce = {
    reduceoutputvalue &lt;- rbind(reduceoutputvalue, do.call(rbind, reduce.values))
  },
  post = {
    rhcollect(reduce.key, reduceoutputvalue)
  }
)</code></pre>
<p>The first task of Reduce is to group its input key-value pairs by unique key, in this case by <code>state</code>. The Reduce R code is applied to the key-value pairs of each group by a reducer.</p>
<p>Expression <code>pre</code>, initializes <code>reduceoutputvalue</code> to a <code>data.frame()</code>. <code>reduce</code> assembles the state <code>data.frame</code> as the reducer receives the values through <code>rbind(reduceoutputvalue, do.call(rbind, reduce.values))</code>; this uses <code>rbind()</code> to add rows to the <code>data.frame</code> object. <code>post</code> operates further on the result of <code>reduce</code>; <code>rhcollect()</code> forms a Reduce output key-value pair for each state. RHIPE then writes the Reduce output key-value pairs to the HDFS.</p>
<p>Recall that we told RHIPE in <code>rhwatch()</code> to also write the Reduce output to <code>CountyStats</code> in both the R server global environment. There, we can have a look at the results to make sure all is well. We can look at a summary</p>
<pre><code>str(CountyStats)
List of 49
 $ :List of 2
  ..$ : Named chr &quot;AL&quot;
  .. ..- attr(*, &quot;names&quot;)= chr &quot;state&quot;
  ..$ :&#39;data.frame&#39;:    64 obs. of  5 variables:
  .. ..$ FIPS  : chr [1:64] &quot;01055&quot; &quot;01053&quot; &quot;01051&quot; &quot;01049&quot; ...
  .. ..$ county: chr [1:64] &quot;Etowah&quot; &quot;Escambia&quot; &quot;Elmore&quot; &quot;DeKalb&quot; ...
  .. ..$ min   : num [1:64] 62.1 60.4 94.7 59.2 41.2 ...
  .. ..$ median: num [1:64] 67.6 66.2 99.2 71.9 50.6 ...
  .. ..$ max   : num [1:64] 77.8 79.8 102.2 82.3 60.4 ...
 $ :List of 2
  ..$ : Named chr &quot;AR&quot;
  .. ..- attr(*, &quot;names&quot;)= chr &quot;state&quot;
  ..$ :&#39;data.frame&#39;:    71 obs. of  5 variables:
  .. ..$ FIPS  : chr [1:71] &quot;05025&quot; &quot;05023&quot; &quot;05021&quot; &quot;05019&quot; ...
  .. ..$ county: chr [1:71] &quot;Cleveland&quot; &quot;Cleburne&quot; &quot;Clay&quot; &quot;Clark&quot; ...
  .. ..$ min   : num [1:71] 46.2 99.9 28.1 61.6 58.5 ...
  .. ..$ median: num [1:71] 60.2 108.2 38.7 67.3 82.1 ...
  .. ..$ max   : num [1:71] 73.5 125 48.8 72.7 117.4 ...
......</code></pre>
<p>We can look at the first key-value pair</p>
<pre><code>CountyStats[[1]][[2]]
[[1]]
state
 &quot;AL&quot;</code></pre>
<p>We can look at the <code>data.frame</code> for state “AL”</p>
<pre><code>head(CountyStats[[1]][[2]])
        min    median       max  FIPS     county
1  34.88372  51.88628  73.46257 01093     Marion
2  94.66667  99.20582 102.23077 01051     Elmore
3  83.93817  88.59977  94.67041 01031     Coffee
4  92.87617  97.53306 105.71429 01125 Tuscaloosa
5  60.34774  72.46377  93.53741 01027       Clay
6 108.97167 119.66207 128.13390 01117     Shelby</code></pre>
</div>
</div>
</div>


      </div>
    </div>
  </div>

  <div id="footer">
    <div class="container">
      <div class="col-md-6">
              </div>
      <div class="col-md-6">
        <p class="pull-right">created with <a href="https://github.com/hafen/packagedocs">packagedocs</a></p>
      </div>
    </div>
  </div>


</body>
</html>
