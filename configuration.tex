\subsection{Configuration}
Hadoop's HDFS will use space on the local disks on node002 through
node005 to create a single large distributed filesystem. In our example
we will say each of node002 through node005 has four 2TB drives per
node, for a total of 32TB across the four servers.  These local drives
must be already been formatted by Linux as normal Linux filesystems
(ext4, ext3, etc.).  We will further say these filesystems have been
mounted with the same mount point locations on all nodes: /hadoop/disk1,
/hadoop/disk2, /hadoop/disk3, and /hadoop/disk4.  HDFS actually permits
different sized drives and different numbers of drives on each server,
but doing so requires each server to have an individually maintained
hdfs-site.xml file, described later.  If possible, keep the number of
drives the same on each node for easier administration.  The names of
the mount points aren't special and could be anything, but these are
the names we will use in this example.  HDFS creates its own directory
structure and files within the drives that are part of HDFS.  If one
were to look inside one of these filesystems after Hadoop is running
and files have been written to HDFS, one would see a file and directory
structure with computer generated directory names and filenames that
don't correspond in any way to their HDFS filenames (eg. blk\_1073742354,
blk\_1073742354\_1530.meta, etc.).  Access to these files must be made
through Hadoop.  Paths to files in HDFS begin with `\!\ /\!\ ' just as
they do on a normal Linux filesystem, but the namespace is completely
independent from all other files on the nodes.  This means the directory
/tmp on HDFS is completely different from the normal /tmp visible at
a bash prompt.  Some paths in the configuration below are in the Linux
filesystem namespace an some are in the HDFS filesystem namespace.

The frontend server does not directly participate in Hadoop computation,
HDFS storage, etc. but it must have knowledge of the Hadoop configuration
in order to interact with Hadoop.  Thus all Hadoop configuration files
must exist on the frontend in addition to the nodeNNN servers.

The amount of RAM per node must be known for some configuration settings,
so lets say we have 64GB per node in our example configuration.  We will
say each node has 16 CPU cores.


\subsubsection{Hosts File or DNS entries}
The hostnames and IP addresses of all machines in the Hadoop cluster must
be added to the \verb|/etc/hosts| file on each machine of the cluster,
or be know to the DNS server each of the nodes uses.  These are our
example hostnames and IP addresses.

\begin{verbatim}
10.0.0.001 node001.example.com node001
10.0.0.002 node002.example.com node002
10.0.0.003 node003.example.com node003
10.0.0.004 node004.example.com node004
10.0.0.005 node005.example.com node005
10.0.0.006 frontend.example.com frontend
\end{verbatim}

\subsubsection{Firewalls}
The new network services for Hadoop, including access to files in HDFS,
will be visible to the Internet unless already blocked due to NAT or
some other form of firewall elsewhere on the network.  It is advised
that a host-based firewall be erected using iptables on each of the
nodes and the fronend server.  It would permit full communication for
all ports among the six servers in our example: node001 through node005
and the frontend server.  It would restrict access to these servers
from external hosts based on your local needs.  For example, you may
wish to permit ssh connections to the frontend from anywhere in
the world, but allow ssh to the nodeNNN servers only from local subnets.
You may want to permit Rstudio Server connections to the frontend
(port 8787) from just three local subnets only, but permit no other
network access to any other servers.  Other ports you may wish to open
to local subnets include port 50070 on node001 to see the status of
HDFS, and port 8088 on node001 to see the status of the ResourceManager,
the jobs that are running, have been completed, etc.

\subsubsection{Hadoop core-site.xml}
The \verb|core-site.xml| file is placed in \verb|/etc/hadoop/conf.tessera| on every server in the cluster (nodeNNN and frontend).  If a change is ever made to this config file, it must be made to that file on every node in the cluster as well.
\begin{verbatim}
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  <property>
    <!-- This is the NameNode -->
    <name>fs.defaultFS</name>
    <value>hdfs://node001.example.com:8020</value>
  </property>
</configuration>
\end{verbatim}


\subsubsection{Hadoop mapred-site.xml}
The \verb|mapred-site.xml| file in in \verb|/etc/hadoop/conf.tessera|
is configured to tell Hadoop we are using the MRv2 / YARN framework.
This file should be copied to all servers (nodeNNN and frontend).

\begin{verbatim}
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
</configuration>
\end{verbatim}


\subsubsection{Hadoop yarn-site.xml}
The \verb|yarn-site.xml| file in in \verb|/etc/hadoop/conf.tessera|
is used to configure Hadoop settings related to temporary storage, number
of CPU cores to use per node, memory per node, etc.   This file should
be copied to all servers (nodeNNN and frontend).  Changes to this file
can have significant performance impact, such as running 16 tasks per
node rather than running 2 per node.  Tuning these settings optimally
is beyond the scope of this guide.  In our example cluster, the hardware
configuration of all nodes is identical, thus the same yarn-site.xml file
can be copied around to all nodes.  If in your configuration you have some
nodes with different amounts of RAM or different numbers of CPU cores,
they must have an individually maintained and updated \verb|yarn-site.xml|
file with those settings configured differently.

\begin{verbatim}
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  <property>
    <!-- This should match the name of the resource manager in your local deployment -->
    <name>yarn.resourcemanager.hostname</name>
    <value>node001.example.com</value>
  </property>

  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>

  <property>
    <!-- How much RAM on this server can be used for Hadoop -->
    <!-- We will use (total RAM - 2GB).  We have 64GB in our example, so use 62GB -->
    <name>yarn.nodemanager.resource.memory-mb</name>
    <value>62000</value>
  </property>

  <property>
    <!-- How many CPU cores on this server can be used for Hadoop -->
    <!-- We will use them all, which is 16 per node in our example cluster -->
    <name>yarn.nodemanager.resource.cpu-vcores</name>
    <value>16</value>
  </property>

  <property>
    <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
  </property>

  <property>
    <name>yarn.resourcemanager.scheduler.class</name>
    <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>
  </property>

  <property>
    <name>yarn.log-aggregation-enable</name>
    <value>true</value>
  </property>

  <property>
    <!-- List of directories to store temporary localized files. -->
    <!-- Spread these across all local drives on all nodes -->
    <name>yarn.nodemanager.local-dirs</name>
    <value>file:///hadoop/disk1/yarn/local,file:///hadoop/disk2/yarn/local,
           file:///hadoop/disk3/yarn/local,file:///hadoop/disk4/yarn/local</value>
  </property>

  <property>
    <!-- Where to store temporary container logs. -->
    <!-- Spread these across all local drives on all nodes -->
    <name>yarn.nodemanager.log-dirs</name>
    <value>file:///hadoop/disk1/yarn/log,file:///hadoop/disk2/yarn/log,
           file:///hadoop/disk3/yarn/log,file:///hadoop/disk4/yarn/log</value>
  </property>

  <property>
    <!-- This should match the name of the NameNode in your local deployment -->
    <name>yarn.nodemanager.remote-app-log-dir</name>
    <value>hdfs://node001.example.com/var/log/hadoop-yarn/apps</value>
  </property>

  <property>
    <name>yarn.application.classpath</name>
    <value>
       $HADOOP_CONF_DIR,
       $HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,
       $HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,
       $HADOOP_MAPRED_HOME/*,$HADOOP_MAPRED_HOME/lib/*,
       $HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*
    </value>
  </property>
</configuration>
\end{verbatim}


We reference new directories above in the local Linux filesystem into which
logs and other files are placed.  We must create these directories now
and set the proper owner, group, and permissions.  This must be done on
node001 - node005:

\begin{verbatim}
sudo mkdir -p /hadoop/disk1/yarn/local /hadoop/disk2/yarn/local
sudo mkdir -p /hadoop/disk3/yarn/local /hadoop/disk4/yarn/local
sudo mkdir -p /hadoop/disk1/yarn/log /hadoop/disk2/yarn/log
sudo mkdir -p /hadoop/disk3/yarn/log /hadoop/disk4/yarn/log
sudo chown -R yarn.yarn /hadoop/disk1/yarn/local /hadoop/disk2/yarn/local
sudo chown -R yarn.yarn /hadoop/disk3/yarn/local /hadoop/disk4/yarn/local
sudo chown -R yarn.yarn /hadoop/disk1/yarn/log /hadoop/disk2/yarn/log
sudo chown -R yarn.yarn /hadoop/disk3/yarn/log /hadoop/disk4/yarn/log
\end{verbatim}

\subsubsection{Hadoop secondary namenode file masters}
The file \verb|/etc/hadoop/conf.tessera/masters| must contain the hostname
of the secondary namenode.  This file should be copied to all servers.
For our example, it contains just one line:

\begin{verbatim}
node002.example.com
\end{verbatim}

\subsubsection{Hadoop hdfs-site.xml}
The \verb|hdfs-site.xml| file is used to configure the
Hadoop file system (HDFS).   This file should be copied to the
\verb|/etc/hadoop/conf.tessera| directory on all nodes (nodeNNN and
frontend).  As with \verb|yarn-site.xml|, if your number of disks and
size of disk varies among nodes, you must independently maintain copies
of \verb|hdfs-site.xml| on each node.

\begin{verbatim}
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  <property>
    <!--  Number of times each HDFS block is replicated.  Default is 3. -->
    <name>dfs.replication</name>
    <value>3</value>
  </property>

  <property>
    <!-- Size in bytes of each HDFS block.  Should be a power of 2. -->
    <!--  We use 2^27 -->
    <name>dfs.blocksize</name>
    <value>134217728</value>
  </property>

  <!-- Where the namenode stores HDFS metadata on its local drives -->
  <!-- These are Linux filesystem paths that must already exist. -->
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:///hadoop/disk1/dfs/nn,file:///hadoop/disk2/dfs/nn,
           file:///hadoop/disk3/dfs/nn,file:///hadoop/disk4/dfs/nn</value>
  </property>

  <!-- Where the secondary namenode stores HDFS metadata on its local drives -->
  <!-- These are Linux filesystem paths that must already exist. -->
  <property>
    <name>dfs.namenode.checkpoint.dir</name>
    <value>file:///hadoop/disk1/dfs/nn,file:///hadoop/disk2/dfs/nn,
           file:///hadoop/disk3/dfs/nn,file:///hadoop/disk4/dfs/nn</value>
  </property>

  <property>
    <!-- Where each datanode stores HDFS blocks on its local drives. -->
    <!-- These are Linux filesystem paths that must already exist. -->
    <name>dfs.datanode.data.dir</name>
    <value>file:///hadoop/disk1/dfs/dn,file:///hadoop/disk2/dfs/dn,
           file:///hadoop/disk3/dfs/dn,file:///hadoop/disk4/dfs/dn</value>
  </property>

  <property>
    <!-- This should match the name of the NameNode in your local deployment -->
    <name>dfs.namenode.http-address</name>
    <value>node001.example.com:50070</value>
  </property>

  <property>
     <name>dfs.permissions.superusergroup</name>
     <value>hadoop</value>
  </property>
  <property>
    <name>dfs.client.read.shortcircuit</name>
    <value>true</value>
  </property>
  <property>
    <name>dfs.client.read.shortcircuit.streams.cache.size</name>
    <value>1000</value>
  </property>
  <property>
    <name>dfs.client.read.shortcircuit.streams.cache.expiry.ms</name>
    <value>10000</value>
  </property>

  <property>
    <!-- Leave the dn._PORT as is, do not try to make this a number -->
    <name>dfs.domain.socket.path</name>
    <value>/var/run/hadoop-hdfs/dn._PORT</value>
  </property>
</configuration>
\end{verbatim}


We must pre-create the local metadata storage directories on the NameNode
(node001), and on the Secondary NameNode (node002):

\begin{verbatim}
sudo mkdir -p /hadoop/disk1/dfs/nn /hadoop/disk2/dfs/nn
sudo mkdir -p /hadoop/disk3/dfs/nn /hadoop/disk4/dfs/nn 
sudo chown -R hdfs.hdfs /hadoop/disk1/dfs /hadoop/disk2/dfs
sudo chown -R hdfs.hdfs /hadoop/disk3/dfs /hadoop/disk4/dfs
sudo chmod 700 /hadoop/disk1/dfs /hadoop/disk2/dfs /hadoop/disk3/dfs /hadoop/disk4/dfs
\end{verbatim}

We must pre-create the local storage directories on the datanodes, node002
through node005:

\begin{verbatim}
sudo mkdir -p /hadoop/disk1/dfs/dn /hadoop/disk2/dfs/dn
sudo mkdir -p /hadoop/disk3/dfs/dn /hadoop/disk4/dfs/dn
sudo chown -R hdfs.hdfs /hadoop/disk1/dfs /hadoop/disk2/dfs
sudo chown -R hdfs.hdfs /hadoop/disk3/dfs /hadoop/disk4/dfs
sudo chmod 700 /hadoop/disk1/dfs /hadoop/disk2/dfs
sudo chmod 700 /hadoop/disk3/dfs /hadoop/disk4/dfs
\end{verbatim}
